{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# O-information and its derivatives for network behavior and encoding\n\nThis example illustrates how to use the O-information and how it's linked to\nother metrics such as the Total Correlation (TC), the Dual Total Correlation\n(DTC), the S-Information and the gradient O-information. We recommend reading\nRosas et al. 2019 :cite:`rosas2019oinfo`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom hoi.metrics import Oinfo, TC, DTC, Sinfo, GradientOinfo\nfrom hoi.utils import get_nbest_mult\n\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"ggplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data simulation\n\nLet's first simulate some data to showcase the different metrics. Here, we\nare going to simulate a network of 6 nodes, then we introduce redudancy\nbetween nodes (0, 1, 2) and synergy between nodes (3, 4, 5) :\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# network of 6 nodes and 1000 samples each\nx = np.random.rand(1000, 6)\n\n# inject redundancy between nodes (0, 1, 2)\nx[:, 1] += x[:, 0]\nx[:, 2] += x[:, 0]\n\n# inject synergy between nodes (3, 4, 5)\nx[:, 3] += x[:, 4] + x[:, 5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## O-information for network behavior\n\nThe O-information is a multivariate measure of information capable of\ndisentangling whether a subset of a variable X tend to have synergistic or\nredundant interactions. The O-information is defined as the difference\nbetween two quantities, the Total Correlation (TC) and Dual Total Correlation\n(DTC).\n\n### Total Correlation (TC)\n\nThe TC is defined as :\n\n\\begin{align}TC(X^{n}) = \\sum_{j=1}^{n} H(X_{j}) - H(X^{n})\\end{align}\n\nTo give an intuition about the TC, the $\\sum_{j=1}^{n} H(X_{j})$\nquantifies the amount of information carried by individual nodes in a system\nwhile $H(X^{n})$ also contains the information carried by individual\nnode plus their interactions. Then, by taking the subtraction between the two\nquantities we isolate the information contains in the interactions. That's\nwhy the TC quantifies the strength of the \"collective constraints\" which is\nrelated to redundancy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# compute hoi using TC :\nmodel = TC(x)\nhoi = model.fit(method=\"gc\", minsize=3, maxsize=3)\n\n# get the multiplets with largest values of hoi\nprint(get_nbest_mult(hoi, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see from the table above, the TC found the multiplet (0, 1, 2),\ndefined as a redundant triplet with the largest value.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dual Total Correlation (DTC)\n\nThe DTC is defined as :\n\n\\begin{align}DTC(X^{n}) = H(X^{n}) - \\sum_{j=1}^{n} H(X_j|X_{-j}^{n})\\end{align}\n\nAgain, the DTC is also defined as the difference between two terms. The first\none, $H(X^{n})$, just as in the TC, quantifies the amount of\ninformation of the whole system (i.e. the information of individual nodes and\ntheir interactions). To this quantity, we subtract the term\n$\\sum_{j=1}^{n} H(X_j|X_{-j}^{n})$ which represents the sum of\ninformation of individual nodes ($\\sum_{j=1}^{n} H(X_j)$) to which we\nremove the influence of all other nodes except the one concerned\n($|X_{-j}^{n}$). Said differently, this conditioning allows to isolate\nthe intrinsic entropy of each node $X_{j}$ that is not shared with\nothers. Consequently, this difference isolates the \"shared randomness\" which\nis linked to the synergy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# compute hoi using DTC :\nmodel = DTC(x)\nhoi = model.fit(method=\"gc\", minsize=3, maxsize=3)\n\n# get the multiplets with largest values of hoi\nprint(get_nbest_mult(hoi, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time, the DTC finds the triplet (3, 4, 5) as the one with the largest\nvalue of HOI and it's normal because this triplet is synergistic and the DTC\nis related to synergy.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### O-information\n\nFinally, the O-information is defined as the difference between the TC and\nDTC. As both quantities are respectively linked to redundancy and synergy,\nthe O-information is going to be positive when a system is dominated by\nredundant interactions and negative when the system is dominated by\nsynergistic interactions. The mathematical definition of the O-information is\ngiven by :\n\n\\begin{align}\\Omega(X^{n})  &=  TC(X^{n}) - DTC(X^{n}) \\\\\n                   &=  (n - 2)H(X^{n}) + \\sum_{j=1}^{n} [H(X_{j}) - H(\n                        X_{-j}^{n})]\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# compute hoi using the O-information :\nmodel = Oinfo(x)\nhoi = model.fit(method=\"gc\", minsize=3, maxsize=3)\n\n# get the multiplets with largest values of hoi\nprint(get_nbest_mult(hoi, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the O-information isolates the triplet (0, 1, 2) as the one with the\nlargest positive, redundant, hoi and the triplet (3, 4, 5) as the one with\nthe smallest, negative and therefore synergistic multiplet.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### S-information\nFinally, the S-information is defined as the sum of the TC and DTC :\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. math::\n#     \\Omega(X^{n})  &=  TC(X^{n}) + DTC(X^{n}) \\\\\n#                    &=  nH(X^{n}) + \\sum_{j=1}^{n} [H(X_{j}) + H(\n#                    X_{-j}^{n})]\n\n# compute hoi using the S-information :\nmodel = Sinfo(x)\nhoi = model.fit(method=\"gc\", minsize=3, maxsize=3)\n\n# get the multiplets with largest values of hoi\nprint(get_nbest_mult(hoi, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The S-info isolates the two triplets (0, 1, 2) and (3, 4, 5), even if both\nhave different characters (redundant and synergistic)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## O-information for network encoding\n\nThe previous section focused on network behavior, i.e. characterizing the\ntype of interactions between the elements of a system. But what if we want\nnow to add a target variable, i.e. ask the question whether a multiplet is\ncarrying redundant or synergistic information **about** a target variable\n$Y$. Intuitively, we could quantify the difference between the\ninformation carried by all of the nodes $X_{j}$ with target minus the\ninformation of all of the nodes without the target. This is the idea behind\nthe gradient O-information, defined as the difference between the two\nO-information :\n\n\\begin{align}\\partial_{i}\\Omega(X^{n}) &= \\Omega(X^{n}) - \\Omega(X^{n}_{-i}) \\\\\n                                &= (2 - n)I(X_{i}; X^{n}_{-i}) + \\sum_{\n                                k=1, k\\neq i}^{n} I(X_{k}; X^{n}_{-ik})\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulating redundant encoding\nTo simulate redundant encoding, we send a copy of $Y$ to the nodes\n(0, 1, 2)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# network of 6 nodes and 1000 samples each\nx = np.random.rand(1000, 6)\n\n# target variable\ny = np.random.rand(1000)\n\n# redundancy between nodes (0, 1, 2) about y\nx[:, 0] += y\nx[:, 1] += y\nx[:, 2] += y\n\n# compute gradient o-info :\nmodel = GradientOinfo(x, y)\nhoi = model.fit(method=\"gc\", minsize=3, maxsize=3)\n\n# get the multiplets with largest values of hoi\nprint(get_nbest_mult(hoi, model, n_best=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient O-info retrieved redundant interactions between nodes (0, 1, 2)\nabout $Y$\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulating synergistic encoding\nTo simulate synergistic encoding, $Y$ is going to be defined as the sum\nbetween nodes (3, 4, 5)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# network of 6 nodes and 1000 samples each\nx = np.random.rand(1000, 6)\n\n# synergy between nodes (3, 4, 5) about y\ny = x[:, 3] + x[:, 4] + x[:, 5]\n\n# compute gradient o-info :\nmodel = GradientOinfo(x, y)\nhoi = model.fit(method=\"gc\", minsize=3, maxsize=3)\n\n# get the multiplets with largest values of hoi\nprint(get_nbest_mult(hoi, model, n_best=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient O-info retrieved synergistic interactions between nodes\n(3, 4, 5) about $Y$\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combining redundant and synergistic codings using a multivariate target\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# network of 6 nodes and 1000 samples each\nx = np.random.rand(1000, 6)\n\n# define a bivariate target variable\ny = np.random.rand(1000, 2)\n\n# redundancy between nodes (0, 1, 2) about the first column of y\nx[:, 0] += y[:, 0]\nx[:, 1] += y[:, 0]\nx[:, 2] += y[:, 0]\n\n# synergy between nodes (3, 4, 5) about the second column of y\ny[:, 1] += x[:, 3] + x[:, 4] + x[:, 5]\n\n# compute gradient o-info :\nmodel = GradientOinfo(x, y)\nhoi = model.fit(method=\"gc\", minsize=3, maxsize=3)\n\n# get the multiplets with largest values of hoi\nprint(get_nbest_mult(hoi, model, n_best=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient O-info retrieved the redundant interactions between (0, 1, 2)\nand synergistic interactions between (3, 4, 5) about $Y$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dynamic redundant and synergistic codings\nFinally, we can compute the O-information and Gradient O-information on\ndynamic data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# network of 6 nodes, 1000 samples each and a 100 time points\nx = np.random.rand(1000, 4, 100)\n\n# define a dynamic target variable\ny = np.random.rand(1000, 1, 100)\n\n# define a hanning window\nwin = np.hanning(30)\n\n# redundancy between nodes (0, 1, 2) about :math:`Y` between samples [20, 50]\nx[:, 0, 20:50] += y[:, 0, 20:50] * win\nx[:, 1, 20:50] += y[:, 0, 20:50] * win\nx[:, 2, 20:50] += y[:, 0, 20:50] * win\n\n# synergy between nodes (1, 2, 3) about :math:`Y` between samples [50, 80]\ny[:, 0, 50:80] += (x[:, 1, 50:80] + x[:, 2, 50:80] + x[:, 3, 50:80]) * win\n\nmodel = GradientOinfo(x, y)\nhoi = model.fit(method=\"gc\", minsize=3, maxsize=3)\n\nfor n_m, m in enumerate(model.multiplets):\n    plt.plot(hoi[n_m], label=str(m))\nplt.xlabel(\"Times\")\nplt.ylabel(\"Gradient O-info [bits]\")\nplt.title(\"Dynamic gradient O-information\", fontweight=\"bold\")\nplt.legend()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
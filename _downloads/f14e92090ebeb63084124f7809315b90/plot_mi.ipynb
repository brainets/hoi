{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of mutual-information estimators\n\nIn this example, we are going to compare estimators of mutual-information (MI).\nIn particular, the MI between variables sampled from a normal distribution can\nbe estimated theoretically. In this this tutorial we are going\nto :\n\n1. Simulate data sampled from a normal distribution\n2. Define several estimators of MI\n3. Compute the MI for a varying number of samples\n4. See if the computed MI converge toward the theoretical value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom functools import partial\n\nfrom hoi.core import get_mi\nfrom hoi.utils import digitize\n\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"ggplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition of MI estimators\n\nLet us define several estimators of MI. We are going to use the GC MI\n(Gaussian Copula Mutual Information), the KNN (k Nearest Neighbor) and the\nkernel-based estimator, using the a binning approach and the histogram\nestimator. Please note that the histogram estimator is equivalent to the\nbinning, with a correction that relate to the difference between the Shannon\nentropy of discrete variables and the differential entropy of continuous\nvariables. This correction in the case of mutual information (MI) is not\nneeded, because in the operation to compute the MI, the difference between\ndiscrete and differential entropy cancel out.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a special function for the binning approach as it requires binary data\nmi_binning_fcn = get_mi(\"binning\", base=2)\n\n\ndef mi_binning(x, y, **kwargs):\n    x = digitize(x.T, **kwargs).T\n    y = digitize(y.T, **kwargs).T\n\n    return mi_binning_fcn(x, y)\n\n\n# list of estimators to compare\nmetrics = {\n    \"GC\": get_mi(\"gc\", biascorrect=False),\n    \"KNN-3\": get_mi(\"knn\", k=3),\n    \"KNN-10\": get_mi(\"knn\", k=10),\n    \"Kernel\": get_mi(\"kernel\"),\n    \"Binning\": partial(mi_binning, n_bins=4),\n    \"Histogram\": get_mi(\"histogram\", n_bins=4),\n}\n\n# number of samples to simulate data\nn_samples = np.geomspace(20, 1000, 10).astype(int)\n\n# number of repetitions to estimate the percentile interval\nn_repeat = 10\n\n\n# plotting function\ndef plot(mi, mi_theoric):\n    \"\"\"Plotting function.\"\"\"\n    for n_m, metric_name in enumerate(mi.keys()):\n        # get the entropies\n        x = mi[metric_name]\n\n        # get the color\n        color = f\"C{n_m}\"\n\n        # estimate lower and upper bounds of the [5, 95]th percentile interval\n        x_low, x_high = np.percentile(x, [5, 95], axis=0)\n\n        # plot the MI as a function of the number of samples and interval\n        plt.plot(n_samples, x.mean(0), color=color, lw=2, label=metric_name)\n        plt.fill_between(n_samples, x_low, x_high, color=color, alpha=0.2)\n\n    # plot the theoretical value\n    plt.axhline(mi_theoric, linestyle=\"--\", color=\"k\", label=\"Theoretical MI\")\n    plt.legend()\n    plt.xlabel(\"Number of samples\")\n    plt.ylabel(\"Mutual-information [bits]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MI of data sampled from normal distribution\n\nGiven two variables $X \\sim \\mathcal{N}(\\mu_{x}, \\sigma_{x})$ and\n$Y \\sim \\mathcal{N}(\\mu_{y}, \\sigma_{y})$, linked by a covariance\n$C$ the theoretical MI in bits is defined by :\n\n\\begin{align}I(X; Y) = \\frac{1}{2} \\times log_{2}(\\frac{\\sigma_{x}^{2}\\sigma_{y}^{2}}{ \\sigma_{x}^{2}\\sigma_{y}^{2} - C^{2} })\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# mean and standard error of x and y variables\nmu_x = 0.0\nmu_y = 0.0\nsigma_x = 1.0\nsigma_y = 1.0\n\n# covariance between x and y\ncovariance = 0.5\n\n# covariance matrix\ncov_matrix = [[sigma_x**2, covariance], [covariance, sigma_y**2]]\n\n# define the theoretic MI\nmi_theoric = 0.5 * np.log2(\n    sigma_x**2 * sigma_y**2 / (sigma_x**2 * sigma_y**2 - covariance**2)\n)\n\n# compute mi using various metrics\nmi = {k: np.zeros((n_repeat, len(n_samples))) for k in metrics.keys()}\n\nfor n_s, s in enumerate(n_samples):\n    for n_r in range(n_repeat):\n        for metric, fcn in metrics.items():\n            # generate samples from joint gaussian distribution\n            fx = np.random.multivariate_normal([mu_x, mu_y], cov_matrix, s)\n\n            # extract x and y\n            x = fx[:, [0]].T\n            y = fx[:, [1]].T\n\n            # compute mi\n            mi[metric][n_r, n_s] = fcn(x, y)\n\n# plot the results\nplot(mi, mi_theoric)\nplt.title(\n    \"Comparison of MI estimators when the\\ndata are sampled from a \"\n    \"normal distribution\",\n    fontweight=\"bold\",\n)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
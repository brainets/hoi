{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of entropy estimators for various distributions\n\nIn this example, we are going to compare entropy estimators. In particular,\nsome distributions, such as normal, uniform or exponential distributions lead\nto specific values of entropies. In this this tutorial we are going to :\n\n1. Simulate data following either a normal, uniform or exponential\n   distributions\n2. Define several estimators of entropy\n3. Compute the entropy for a varying number of samples\n4. See if the computed entropies converge toward the theoretical value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom hoi.core import get_entropy\n\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"ggplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition of estimators of entropy\n\nLet us define several estimators of entropy. We are going to use the GC\n(Gaussian Copula), the KNN (k Nearest Neighbor) and the kernel-based\nestimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# list of estimators to compare\nmetrics = {\n    \"GC\": get_entropy(\"gc\"),\n    \"Gaussian\": get_entropy(\"gauss\"),\n    \"KNN-3\": get_entropy(\"knn\", k=3),\n    \"Kernel\": get_entropy(\"kernel\"),\n}\n\n# number of samples to simulate data\nn_samples = np.geomspace(20, 1000, 15).astype(int)\n\n# number of repetitions to estimate the percentile interval\nn_repeat = 10\n\n\n# plotting function\ndef plot(h_x, h_theoric):\n    \"\"\"Plotting function.\"\"\"\n    for n_m, metric_name in enumerate(h_x.keys()):\n        # get the entropies\n        x = h_x[metric_name]\n\n        # get the color\n        color = f\"C{n_m}\"\n\n        # estimate lower and upper bounds of the [5, 95]th percentile interval\n        x_low, x_high = np.percentile(x, [5, 95], axis=0)\n\n        # plot the entropy as a function of the number of samples and interval\n        plt.plot(n_samples, x.mean(0), color=color, lw=2, label=metric_name)\n        plt.fill_between(n_samples, x_low, x_high, color=color, alpha=0.2)\n\n    # plot the theoretical value\n    plt.axhline(\n        h_theoric, linestyle=\"--\", color=\"k\", label=\"Theoretical entropy\"\n    )\n    plt.legend()\n    plt.xlabel(\"Number of samples\")\n    plt.ylabel(\"Entropy [bits]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entropy of data sampled from normal distribution\n\nFor data sampled from a normal distribution of mean `m` and standard\ndeviation of \u03c3 ($X \\sim \\mathcal{N}(0, 1)$), the theoretical entropy is\ndefined by :\n\n\\begin{align}H(X) = \\frac{1}{2} \\times log_{2}(2\u03c0e\u03c3^2)\\end{align}\n\nwith `e` the Euler constant.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# mean and standard error N(0, 1)\nmu = 0.0\nsigma = 1.0\n\n# define the theoretic entropy\nh_theoric = 0.5 * np.log2(2 * np.pi * np.e * (sigma**2))\n\n# compute entropies using various metrics\nh_x = {k: np.zeros((n_repeat, len(n_samples))) for k in metrics.keys()}\n\nfor metric, fcn in metrics.items():\n    for n_s, s in enumerate(n_samples):\n        for n_r in range(n_repeat):\n            x = np.random.normal(mu, sigma, size=(s,)).reshape(1, -1)\n            h_x[metric][n_r, n_s] = fcn(x)\n\n# plot the results\nplot(h_x, h_theoric)\nplt.title(\n    \"Comparison of entropy estimators when the\\ndata are sampled from a \"\n    \"normal distribution\",\n    fontweight=\"bold\",\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entropy of data sampled from a uniform distribution\n\nFor data sampled from a uniform distribution defined between bounds\n$[a, b]$ ($X \\sim \\mathcal{U}(a, b)$), the theoretical entropy is\ndefined by :\n\n\\begin{align}H(X) = log_{2}(b - a)\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# boundaries\na = 31\nb = 107\n\n# define the theoretic entropy\nh_theoric = np.log2(b - a)\n\n# compute entropies using various metrics\nh_x = {k: np.zeros((n_repeat, len(n_samples))) for k in metrics.keys()}\n\nfor metric, fcn in metrics.items():\n    for n_s, s in enumerate(n_samples):\n        for n_r in range(n_repeat):\n            x = np.random.uniform(a, b, size=(s,)).reshape(1, -1)\n            h_x[metric][n_r, n_s] = fcn(x)\n\n# plot the results\nplot(h_x, h_theoric)\nplt.title(\n    \"Comparison of entropy estimators when the\\ndata are sampled from a \"\n    \"uniform distribution\",\n    fontweight=\"bold\",\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entropy of data sampled from an exponential distribution\n\nFor data sampled from an exponential distribution defined by its rate\n$lambda$, the theoretical entropy is defined by :\n\n\\begin{align}H(X) = log(e / \\lambda)\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# lambda parameter\nlambda_ = 0.2\n\n# define the theoretic entropy\nh_theoric = np.log2(np.e / lambda_)\n\n# compute entropies using various metrics\nh_x = {k: np.zeros((n_repeat, len(n_samples))) for k in metrics.keys()}\n\nfor metric, fcn in metrics.items():\n    for n_s, s in enumerate(n_samples):\n        for n_r in range(n_repeat):\n            x = np.random.uniform(a, b, size=(s,)).reshape(1, -1)\n            x = np.random.exponential(1 / lambda_, size=(1, s)).reshape(1, -1)\n            h_x[metric][n_r, n_s] = fcn(x)\n\n# plot the results\nplot(h_x, h_theoric)\nplt.title(\n    \"Comparison of entropy estimators when the\\ndata are sampled from a \"\n    \"exponential distribution\",\n    fontweight=\"bold\",\n)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of entropy estimators with high-dimensional data\n\nIn this example, we are going to compare estimators of entropy with\nhigh-dimensional data.\n\n1. Simulate data sampled from a multivariate normal distribution.\n2. Define estimators of entropy.\n3. Compute the entropy for a varying number of samples.\n4. See if the estimated entropy converge towards the theoretical value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom hoi.core import get_entropy\n\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"ggplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition of entropy estimators\n\nWe are going to use the GCMI (Gaussian Copula Mutual Information), KNN\n(k Nearest Neighbor), a Gaussian kernel-based estimator and the histogram\nestimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# list of estimators to compare\nmetrics = {\n    \"GCMI\": get_entropy(\"gc\", biascorrect=False),\n    \"KNN-3\": get_entropy(\"knn\", k=3),\n    \"KNN-10\": get_entropy(\"knn\", k=10),\n    \"Kernel\": get_entropy(\"kernel\"),\n    \"Histogram\": get_entropy(\"histogram\"),\n}\n\n# number of samples to simulate data\nn_samples = np.geomspace(20, 1000, 15).astype(int)\n\n# number of repetitions to estimate the percentile interval\nn_repeat = 10\n\n\n# plotting function\ndef plot(ent, ent_theoric, ax):\n    \"\"\"Plotting function.\"\"\"\n    for n_m, metric_name in enumerate(ent.keys()):\n        # get the entropies\n        x = ent[metric_name]\n\n        # get the color\n        color = f\"C{n_m}\"\n\n        # estimate lower and upper bounds of the [5, 95]th percentile interval\n        x_low, x_high = np.percentile(x, [5, 95], axis=0)\n\n        # plot the MI as a function of the number of samples and interval\n        ax.plot(n_samples, x.mean(0), color=color, lw=2, label=metric_name)\n        ax.fill_between(n_samples, x_low, x_high, color=color, alpha=0.2)\n\n    # plot the theoretical value\n    ax.axhline(\n        ent_theoric, linestyle=\"--\", color=\"k\", label=\"Theoretical entropy\"\n    )\n    ax.legend()\n    ax.set_xlabel(\"Number of samples\")\n    ax.set_ylabel(\"Entropy [bits]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##############################################################################\n Entropy of data sampled from multinormal distribution\n -------------------------------------------\n\n Let variables $X_1,X_2,...,X_n$ have a multivariate normal distribution\n $\\mathcal{N}(\\vec{\\mu}, \\Sigma)$ the theoretical entropy in bits is\n defined by :\n\n .. math::\n     H(X) = \\frac{1}{2} \\times log_{2}({|\\Sigma|}(2\\pi e)^{n})\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# function for creating the covariance matrix with differnt modes\ndef create_cov_matrix(n_dims, cov, mode=\"dense\", k=None):\n    \"\"\"Create a covariance matrix.\"\"\"\n    # variance 1 for each dim\n    cov_matrix = np.eye(n_dims)\n    if mode == \"dense\":\n        # all dimensions, but diagonal, with covariance cov\n        cov_matrix += cov\n        cov_matrix[np.diag_indices(n_dims)] = 1\n    elif mode == \"sparse\":\n        # only pairs x_i, x_(i+1) with i < k have covariance cov\n        k = k if k is not None else n_dims\n        for i in range(n_dims - 1):\n            if i < k:\n                cov_matrix[i, i + 1] = cov\n                cov_matrix[i + 1, i] = cov\n\n    return cov_matrix\n\n\ndef compute_true_entropy(cov_matrix):\n    \"\"\"Compute the true entropy (bits).\"\"\"\n    n_dims = cov_matrix.shape[0]\n    det_cov = np.linalg.det(cov_matrix)\n    return 0.5 * np.log2(det_cov * (2 * np.pi * np.e) ** n_dims)\n\n\n# number of dimensions per variable\nn_dims = 4\n# mean\nmu = [0.0] * n_dims\n# covariance\ncovariance = 0.6\n\n# modes for the covariance matrix:\n# - dense: off diagonal elements have specified covariance\n# - sparse: only pairs xi, x_(i+1) with i < k have specified covariance\nmodes = [\"dense\", \"sparse\"]\n# number of pairs with specified covariance\nk = n_dims\n\nfig = plt.figure(figsize=(10, 5))\n# compute entropy using various metrics\nentropy = {k: np.zeros((n_repeat, len(n_samples))) for k in metrics.keys()}\nfor i, mode in enumerate(modes):\n    cov_matrix = create_cov_matrix(n_dims, covariance, mode=mode)\n    # define the theoretic entropy\n    ent_theoric = compute_true_entropy(cov_matrix)\n    ax = fig.add_subplot(1, 2, i + 1)\n\n    for n_s, s in enumerate(n_samples):\n        for n_r in range(n_repeat):\n            # generate samples from joint gaussian distribution\n            fx = np.random.multivariate_normal(mu, cov_matrix, s)\n            for metric, fcn in metrics.items():\n                # extract x and y\n                x = fx[:, :n_dims].T\n                y = fx[:, n_dims:].T\n                # compute entropy\n                entropy[metric][n_r, n_s] = fcn(x)\n\n    # plot the results\n    plot(entropy, ent_theoric, ax)\n    ax.title.set_text(f\"Mode: {mode}\")\n\nfig.suptitle(\n    \"Comparison of entropy estimators when\\nthe data is high-dimensional\",\n    fontweight=\"bold\",\n)\nfig.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
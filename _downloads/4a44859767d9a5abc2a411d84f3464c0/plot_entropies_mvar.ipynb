{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of entropy estimators for a multivariate normal\n\nWhen calculating Higher-Order Interactions using entropy-based metrics, the\nprocess includes estimating the entropy of multivariate variables.\nNevertheless, certain entropy estimators may not function optimally as the\nnumber of dimensions increases. This can be demonstrated through an example\nwhere various entropy estimators are compared using data sampled from a\nmultivariate normal distribution with increasing dimensionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.datasets import make_spd_matrix\n\nfrom hoi.core import get_entropy\n\nplt.style.use(\"ggplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition of estimators of entropy\n\nLet us define several estimators of entropy. We are going to use the GC\n(Gaussian Copula), the KNN (k Nearest Neighbor), the kernel-based, the\nGaussian estimators and the histogram estimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# list of estimators to compare\nmetrics = {\n    \"GC\": get_entropy(\"gc\"),\n    \"Gaussian\": get_entropy(\"gauss\"),\n    \"Histogram\": get_entropy(\"histogram\"),\n    \"KNN-3\": get_entropy(\"knn\", k=3),\n    \"Kernel\": get_entropy(\"kernel\"),\n}\n\n# number of samples to simulate data\nn_samples = np.geomspace(20, 1000, 10).astype(int)\n\n# number of repetitions to estimate the percentile interval\nn_repeat = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define the plotting function. This function plot the entropy as a\nfunction of sample size, for each estimator. It also plots the theoretical\nentropy with an horizontal dotted black line.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# plotting function\ndef plot(h_x, h_theoric, n):\n    \"\"\"Plotting function.\"\"\"\n    for n_m, metric_name in enumerate(h_x.keys()):\n        # get the entropies\n        x = h_x[metric_name]\n\n        # get the color\n        color = f\"C{n_m}\"\n\n        # estimate lower and upper bounds of the [5, 95]th percentile interval\n        x_low, x_high = np.percentile(x, [5, 95], axis=0)\n\n        # plot the entropy as a function of the number of samples and interval\n        plt.plot(n_samples, x.mean(0), color=color, lw=2, label=metric_name)\n        plt.plot(n_samples, x.mean(0), color=color, marker=\"o\")\n        plt.fill_between(n_samples, x_low, x_high, color=color, alpha=0.2)\n\n    # plot the theoretical value\n    plt.axhline(\n        h_theoric, linestyle=\"--\", color=\"k\", label=\"Theoretical entropy\", lw=2\n    )\n    plt.legend()\n    plt.xlabel(\"Number of samples\")\n    plt.ylabel(\"Entropy [bits]\")\n    plt.title(f\"{n}-dimensional multivariate normal\", fontweight=\"bold\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we create a function to generate a covariance matrix that is then going\nto be used for simulating the multivariate normal. The function below takes a\nsingle input describing the dimensionality of the multivariate normal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_covariance(n):\n    \"\"\"Get a symmetric positive semi-definite covariance matrix.\"\"\"\n    # random covariance matrix\n    cov = make_spd_matrix(n)\n\n    # Normalize the covariance matrix to have diagonal elements equal to one\n    d = np.sqrt(np.diag(cov))\n    cov = cov / d[:, None]\n    cov = cov / d[None, :]\n\n    # compute theoretic entropy\n    h = (0.5 * np.log((2 * np.pi * np.e) ** n * np.linalg.det(cov))) / np.log(\n        2\n    )\n\n    return cov, h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we also create a function to compute the entropies, using the\ndefined estimators for an increasing sample size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_entropies(cov):\n    # compute entropies using various metrics\n    h_x = {k: np.zeros((n_repeat, len(n_samples))) for k in metrics.keys()}\n\n    for metric, fcn in metrics.items():\n        for n_s, s in enumerate(n_samples):\n            for n_r in range(n_repeat):\n                x = np.random.multivariate_normal(\n                    [0] * cov.shape[0], cov, size=(s)\n                ).T\n                h_x[metric][n_r, n_s] = fcn(x)\n\n    return h_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of estimators\n\nNow we can make the final figure. Each subplot contains the comparison\nbetween estimators for a n-dimensional multivariate normal. For such\ndistribution, the theoretical entropy is defined as :\n\n\\begin{align}H(X) = \\frac{1}{2} \\times log((2\u03c0e)^{n} |\\Sigma|) / log(2)\\end{align}\n\nwith `e` the Euler constant, $|\\Sigma|$ the determinant of the\ncovariance matrix and `n` the dimensionality of the multivariate normal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n\nplt.sca(axs[0, 0])\nn = 2\ncov, h_theoric = get_covariance(n)\nh_x = compute_entropies(cov)\nplot(h_x, h_theoric, n)\n\nplt.sca(axs[0, 1])\nn = 4\ncov, h_theoric = get_covariance(n)\nh_x = compute_entropies(cov)\nplot(h_x, h_theoric, n)\n\nplt.sca(axs[1, 0])\nn = 6\ncov, h_theoric = get_covariance(n)\nh_x = compute_entropies(cov)\nplot(h_x, h_theoric, n)\n\nplt.sca(axs[1, 1])\nn = 8\ncov, h_theoric = get_covariance(n)\nh_x = compute_entropies(cov)\nplot(h_x, h_theoric, n)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
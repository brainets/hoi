
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Theoretical background &#8212; HOI 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/style.css?v=b7615940" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=55ff85c6"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'theory';</script>
    <link rel="icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="List of classes and functions" href="api/modules.html" />
    <link rel="prev" title="Glossary" href="glossary.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<head>
<style>
@import url('https://fonts.googleapis.com/css2?family=Maven+Pro&family=Roboto+Mono:wght@332&display=swap');

.foot {
    width: 100%;
    text-align: center;
}

/* Optionally, you can add styling to the images as well */
.foot img {
    margin: 0 10px; /* Add some space between the images if desired */
}
</style>
</head>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/hoi-logo.png" class="logo__image only-light" alt="HOI 0.0.1 documentation - Home"/>
    <script>document.write(`<img src="_static/hoi-logo.png" class="logo__image only-dark" alt="HOI 0.0.1 documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/brainets/hoi" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github-square fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://twitter.com/kNearNeighbors" title="Twitter" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-twitter-square fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Twitter</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started 🚀</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">Examples</a></li>





<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Theoretical background</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Further Resources 🔪</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="api/modules.html">Public API: list of functions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="api/api_metrics.html"><code class="docutils literal notranslate"><span class="pre">hoi.metrics</span></code></a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.Oinfo.html">hoi.metrics.Oinfo</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.InfoTopo.html">hoi.metrics.InfoTopo</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.TC.html">hoi.metrics.TC</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.DTC.html">hoi.metrics.DTC</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.Sinfo.html">hoi.metrics.Sinfo</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.RedundancyphiID.html">hoi.metrics.RedundancyphiID</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.SynergyphiID.html">hoi.metrics.SynergyphiID</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.GradientOinfo.html">hoi.metrics.GradientOinfo</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.RSI.html">hoi.metrics.RSI</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.RedundancyMMI.html">hoi.metrics.RedundancyMMI</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.SynergyMMI.html">hoi.metrics.SynergyMMI</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.metrics.InfoTot.html">hoi.metrics.InfoTot</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="api/api_sim.html"><code class="docutils literal notranslate"><span class="pre">hoi.simulation</span></code></a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.simulation.simulate_hoi_gauss.html">hoi.simulation.simulate_hoi_gauss</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="api/api_utils.html"><code class="docutils literal notranslate"><span class="pre">hoi.utils</span></code></a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.utils.digitize.html">hoi.utils.digitize</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.utils.normalize.html">hoi.utils.normalize</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.utils.landscape.html">hoi.utils.landscape</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.utils.get_nbest_mult.html">hoi.utils.get_nbest_mult</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="api/api_plot.html"><code class="docutils literal notranslate"><span class="pre">hoi.plot</span></code></a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.plot.plot_landscape.html">hoi.plot.plot_landscape</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="api/api_core.html"><code class="docutils literal notranslate"><span class="pre">hoi.core</span></code></a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.core.get_entropy.html">hoi.core.get_entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.core.entropy_gc.html">hoi.core.entropy_gc</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.core.entropy_gauss.html">hoi.core.entropy_gauss</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.core.entropy_bin.html">hoi.core.entropy_bin</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.core.entropy_hist.html">hoi.core.entropy_hist</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.core.entropy_knn.html">hoi.core.entropy_knn</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.core.entropy_kernel.html">hoi.core.entropy_kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.core.prepare_for_it.html">hoi.core.prepare_for_it</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.core.get_mi.html">hoi.core.get_mi</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/hoi.core.combinations.html">hoi.core.combinations</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="jax.html">Jax</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributor_guide.html">Developer Documentation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Theoretical background</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Theoretical background</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-information-theoretic-measures">Core information theoretic measures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-entropy">Measuring Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-mutual-information-mi">Measuring Mutual Information (MI)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-pairwise-to-higher-order-interactions">From pairwise to higher-order interactions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#network-behavior">Network behavior</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#total-correlation">Total correlation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dual-total-correlation">Dual Total correlation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#s-information">S information</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#o-information">O-information</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#topological-information">Topological information</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#network-encoding">Network encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-o-information">Gradient of O-information</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#redundancy-synergy-index-rsi">Redundancy-Synergy index (RSI)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#synergy-and-redundancy-mmi">Synergy and redundancy (MMI)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#synergy-and-redundancy-integraed-information-decomposition-mmi">Synergy and redundancy integraed Information Decomposition (MMI)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">Bibliography</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="theoretical-background">
<span id="theory"></span><h1>Theoretical background<a class="headerlink" href="#theoretical-background" title="Link to this heading">#</a></h1>
<p>Since the seminal work of Claude Shannon <span id="id1">[<a class="reference internal" href="#id49" title="Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423, 1948.">27</a>]</span>, in the
second half of the 20th century, <a class="reference internal" href="glossary.html#term-Information-Theory"><span class="xref std std-term">Information Theory</span></a> (IT) has been proven
to be an invaluable framework to decipher the intricate web of interactions
underlying a broad range of different complex systems <span id="id2">[<a class="reference internal" href="#id43" title="Satosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research and development, 4(1):66–82, 1960.">36</a>]</span>.
In this line of research, a plethora of approaches has been developed to
investigate relationships between pairs of variables, shedding light on many
properties of complex systems in a vast range of different fields.
However, a growing body of literature has recently
highlighted that investigating the interactions between groups of more than 2 units,
i.e. <a class="reference internal" href="glossary.html#term-Higher-Order-Interactions"><span class="xref std std-term">Higher Order Interactions</span></a> (HOI), allows to unveil effects that can be
neglected by pairwise approaches <span id="id3">[<a class="reference internal" href="#id48" title="Federico Battiston, Giulia Cencetti, Iacopo Iacopini, Vito Latora, Maxime Lucas, Alice Patania, Jean-Gabriel Young, and Giovanni Petri. Networks beyond pairwise interactions: structure and dynamics. Physics Reports, 874:1–92, 2020.">3</a>]</span>. Hence, how to study
HOI has become a more and more important question in recent times <span id="id4">[<a class="reference internal" href="#id44" title="Federico Battiston, Enrico Amico, Alain Barrat, Ginestra Bianconi, Guilherme Ferraz de Arruda, Benedetta Franceschiello, Iacopo Iacopini, Sonia Kéfi, Vito Latora, Yamir Moreno, and others. The physics of higher-order interactions in complex systems. Nature Physics, 17(10):1093–1098, 2021.">2</a>]</span>.
In this context, new approaches based on IT emerged to investigate HOI in
terms of information content; more into details, different metrics have been developed
to estimate from the activity patterns of a set of variables, whether or not they were
interacting and which kind of interaction they presented
<span id="id5">[<a class="reference internal" href="#id38" title="Nicholas Timme, Wesley Alford, Benjamin Flecker, and John M Beggs. Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective. Journal of computational neuroscience, 36:119–140, 2014.">32</a>, <a class="reference internal" href="#id31" title="Thomas F Varley. Information theory for complex systems scientists. arXiv preprint arXiv:2304.12482, 2023.">35</a>]</span>. Most of these metrics are based on
the concepts of <a class="reference internal" href="glossary.html#term-Synergy"><span class="xref std std-term">Synergy</span></a> and <a class="reference internal" href="glossary.html#term-Redundancy"><span class="xref std std-term">Redundancy</span></a>, formalized in terms of IT by
the <a class="reference internal" href="glossary.html#term-Partial-Information-Decomposition"><span class="xref std std-term">Partial Information Decomposition</span></a> (PID) framework <span id="id6">[<a class="reference internal" href="#id36" title="Paul L Williams and Randall D Beer. Nonnegative decomposition of multivariate information. arXiv preprint arXiv:1004.2515, 2010.">37</a>]</span>.
Even though these metrics are theoretically well defined and fascinating, when concretely using
them to study and computing the higher-order structure of a system, two main problems come into
play: how to estimate entropies and information from limited data set, with different
hypothesis and characteristics, and how to handle the computational cost of such operations.
In our work we provided a complete set of estimators to deal with different kinds of data
sets, e.g. continuous or discrete, and we used the
python library <a class="reference external" href="https://github.com/google/jax">Jax</a> to deal with the high computational
cost. In the following part we will introduce the information theoretical tools necessary
to understand and use the metrics developed in this toolbox. Then we will present quickly
the theory behind the metrics developed, their use and possible interpretation.
Finally, we are going to discuss limitations and future developments of our work.</p>
<section id="core-information-theoretic-measures">
<h2>Core information theoretic measures<a class="headerlink" href="#core-information-theoretic-measures" title="Link to this heading">#</a></h2>
<p>In this section, we delve into some fundamental information theoretic measures, such as
Shannon <a class="reference internal" href="glossary.html#term-Entropy"><span class="xref std std-term">Entropy</span></a> and <a class="reference internal" href="glossary.html#term-Mutual-Information"><span class="xref std std-term">Mutual Information</span></a> (MI), and their applications in
the study of pairwise interactions. Besides the fact that these measures play a crucial
role in various fields such as data science and machine learning, as we will see in the
following parts, they serve as the building blocks for quantifying information and
interactions between variables at higher-orders.</p>
<section id="measuring-entropy">
<h3>Measuring Entropy<a class="headerlink" href="#measuring-entropy" title="Link to this heading">#</a></h3>
<p>Shannon entropy is a fundamental concept in IT, representing the amount of uncertainty
or disorder in a random variable <span id="id7">[<a class="reference internal" href="#id49" title="Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423, 1948.">27</a>]</span>. Its standard definition
for a discrete random variable <span class="math notranslate nohighlight">\(X\)</span>, with probability mass function <span class="math notranslate nohighlight">\(P(X)\)</span>, is given by:</p>
<div class="math notranslate nohighlight">
\[H(X) = −\sum P(x) log_{2}(P(x))\]</div>
<p>However, estimating the probability distribution <span class="math notranslate nohighlight">\(P(X)\)</span> from data can be challenging.
When dealing with a discrete variable that takes values from a limited
set <span class="math notranslate nohighlight">\({x_{1}, x_{2}, ...}\)</span>, one can estimate the probability distribution
by computing the frequencies of each state <span class="math notranslate nohighlight">\(x_{i}\)</span>. In this scenario we estimate
the probability <span class="math notranslate nohighlight">\(P(X=x_{i}) = n_{i}/N\)</span>, where <span class="math notranslate nohighlight">\(n_{i}\)</span> is the number of
occurrences <span class="math notranslate nohighlight">\(X=x_{i}\)</span> and <span class="math notranslate nohighlight">\(N\)</span> is the number of data points. This can present
problems due to size effects when using a small data set and variables exploring a big set of states.</p>
<p>A more complicated and common scenario is the one of continuous variables. To estimate the
entropy of a continuous variable, different methods are implemented in the toolbox:</p>
<ul class="simple">
<li><p>Histogram estimator, that consists in binning the continuous data in a
discrete set of bins. In this way, variables are discretized and the entropy
can be computed as described above, correcting for the bin size .</p></li>
<li><p>Binning method, that allow to extimate the entropy of a discrete variable
estimating the probability of each possible values in a frequentist approach.
Note that this procedure can be performed also on continuous variables after
binarization in many different ways
<span id="id8">[<a class="reference internal" href="#id52" title="Georges A Darbellay and Igor Vajda. Estimation of the information by an adaptive partitioning of the observation space. IEEE Transactions on Information Theory, 45(4):1315–1321, 1999.">7</a>, <a class="reference internal" href="#id51" title="Dominik Endres and Peter Foldiak. Bayesian bin distribution inference and mutual information. IEEE Transactions on Information Theory, 51(11):3766–3779, 2005.">8</a>, <a class="reference internal" href="#id53" title="Andrew M Fraser and Harry L Swinney. Independent coordinates for strange attractors from mutual information. Physical review A, 33(2):1134, 1986.">9</a>]</span>.</p></li>
<li><p>K-Nearest Neighbors (KNN), that estimates the probability distribution by considering the
K-nearest neighbors of each data point <span id="id9">[<a class="reference internal" href="#id57" title="Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004.">15</a>]</span>.</p></li>
<li><p>Kernel Density Estimation that uses kernel functions to estimate the probability
density function, offering a smooth approximation <span id="id10">[<a class="reference internal" href="#id55" title="Young-Il Moon, Balaji Rajagopalan, and Upmanu Lall. Estimation of mutual information using kernel density estimators. Physical Review E, 52(3):2318, 1995.">21</a>]</span>.</p></li>
<li><p>The parametric estimation, that is used when the data is gaussian and allows
to compute the entropy as a function of the variance <span id="id11">[<a class="reference internal" href="#id54" title="Nathaniel R Goodman. Statistical analysis based on a certain multivariate complex gaussian distribution (an introduction). The Annals of mathematical statistics, 34(1):152–177, 1963.">11</a>]</span>.</p></li>
</ul>
<p>Note that all the functions mentioned in the following part are based on the computation of
entropies, hence we advise care in the choice of the estimator to use.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This introduction guides you through the core information theoretical metrics available. These metrics are the entropy and the mutual information."><img alt="" src="_images/sphx_glr_plot_tutorial_core_thumb.png" />
<p><a class="reference internal" href="auto_examples/it/plot_tutorial_core.html#sphx-glr-auto-examples-it-plot-tutorial-core-py"><span class="std std-ref">Introduction to core information theoretical metrics</span></a></p>
  <div class="sphx-glr-thumbnail-title">Introduction to core information theoretical metrics</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="When computing Higher-Order Interactions using metrics based on entropy, it involves estimating entropy of multivariate variables. However, as we are going to see in this example, some estimators of entropy do not perform so well with increasing dimensionality. In this example, to illustrate this effect, we are going to compare entropy estimators with data sampled from a multivariate normal with increasing dimensionality."><img alt="" src="_images/sphx_glr_plot_entropies_mvar_thumb.png" />
<p><a class="reference internal" href="auto_examples/it/plot_entropies_mvar.html#sphx-glr-auto-examples-it-plot-entropies-mvar-py"><span class="std std-ref">Comparison of entropy estimators for a multivariate normal</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of entropy estimators for a multivariate normal</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we are going to compare entropy estimators. In particular, some distributions, such as normal, uniform or exponential distributions lead to specific values of entropies. In this this tutorial we are going to :"><img alt="" src="_images/sphx_glr_plot_entropies_thumb.png" />
<p><a class="reference internal" href="auto_examples/it/plot_entropies.html#sphx-glr-auto-examples-it-plot-entropies-py"><span class="std std-ref">Comparison of entropy estimators for various distributions</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of entropy estimators for various distributions</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we are going to compare estimators of entropy with high-dimensional data."><img alt="" src="_images/sphx_glr_plot_entropy_high_dimensional_thumb.png" />
<p><a class="reference internal" href="auto_examples/it/plot_entropy_high_dimensional.html#sphx-glr-auto-examples-it-plot-entropy-high-dimensional-py"><span class="std std-ref">Comparison of entropy estimators with high-dimensional data</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of entropy estimators with high-dimensional data</div>
</div></div></section>
<section id="measuring-mutual-information-mi">
<h3>Measuring Mutual Information (MI)<a class="headerlink" href="#measuring-mutual-information-mi" title="Link to this heading">#</a></h3>
<p>One of the most used functions in the study of pairwise interaction is the Mutual Information (MI)
that quantifies the statistical dependence or information shared between two random
variables <span id="id12">[<a class="reference internal" href="#id49" title="Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423, 1948.">27</a>, <a class="reference internal" href="#id43" title="Satosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research and development, 4(1):66–82, 1960.">36</a>]</span>. It is defined
mathematically using the concept of entropies. For two random variables X and Y,
MI is given by:</p>
<div class="math notranslate nohighlight">
\[MI(X;Y) = H(X) + H(Y) − H(X,Y)\]</div>
<p>Where:</p>
<p><span class="math notranslate nohighlight">\(H(X)\)</span> and <span class="math notranslate nohighlight">\(H(Y)\)</span> are the entropies of individual variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.
<span class="math notranslate nohighlight">\(H(X,Y)\)</span>  is the joint entropy of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.
MI between two variables, quantifies how much knowing one variable reduces the uncertainty about
the other and measures the interdependency between the two variables. If they are independent,
we have <span class="math notranslate nohighlight">\(H(X,Y)=H(X)+H(Y)\)</span>, hence <span class="math notranslate nohighlight">\(MI(X,Y)=0\)</span>. Since the MI can be reduced to a
signed sum of entropies, the problem of how to estimate MI from continuous data can be
reconducted to the problem, discussed above, of how to estimate entropies. An estimator
that has been recently developed and presents interesting properties when computing the MI
is the Gaussian Copula estimator <span id="id13">[<a class="reference internal" href="#id56" title="Robin AA Ince, Bruno L Giordano, Christoph Kayser, Guillaume A Rousselet, Joachim Gross, and Philippe G Schyns. A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula. Human brain mapping, 38(3):1541–1573, 2017.">13</a>]</span>. This estimator is based on the
statistical theory of copulas and is proven to provide a lower bound to the real value of MI,
this is one of its main advantages: when computing MI, Gaussian copula estimator avoids false
positives. Play attention to the fact that this can be mainly used to investigate relationships
between two variables that are monotonic.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This introduction guides you through the core information theoretical metrics available. These metrics are the entropy and the mutual information."><img alt="" src="_images/sphx_glr_plot_tutorial_core_thumb.png" />
<p><a class="reference internal" href="auto_examples/it/plot_tutorial_core.html#sphx-glr-auto-examples-it-plot-tutorial-core-py"><span class="std std-ref">Introduction to core information theoretical metrics</span></a></p>
  <div class="sphx-glr-thumbnail-title">Introduction to core information theoretical metrics</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we are going to compare estimators of mutual-information (MI). In particular, the MI between variables sampled from a normal distribution can be estimated theoretically. In this this tutorial we are going to :"><img alt="" src="_images/sphx_glr_plot_mi_thumb.png" />
<p><a class="reference internal" href="auto_examples/it/plot_mi.html#sphx-glr-auto-examples-it-plot-mi-py"><span class="std std-ref">Comparison of mutual-information estimators</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of mutual-information estimators</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we are going to compare estimators of mutual-information (MI) with high-dimensional data. In particular, the MI between variables sampled from a multinormal distribution can be estimated theoretically. In this this tutorial we are going to:"><img alt="" src="_images/sphx_glr_plot_mi_high_dimensional_thumb.png" />
<p><a class="reference internal" href="auto_examples/it/plot_mi_high_dimensional.html#sphx-glr-auto-examples-it-plot-mi-high-dimensional-py"><span class="std std-ref">Comparison of MI estimators with high-dimensional data</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of MI estimators with high-dimensional data</div>
</div></div></section>
</section>
<section id="from-pairwise-to-higher-order-interactions">
<h2>From pairwise to higher-order interactions<a class="headerlink" href="#from-pairwise-to-higher-order-interactions" title="Link to this heading">#</a></h2>
<p>The information theoretic metrics involved in this work are all based in principle on the
concept of Shannon entropy and mutual information. Given a set of variables, a common
approach to investigate their interaction is by comparing the entropy and the information
of the joint probability distribution of the whole set with the entropy and information
of different subsets. This can be done in many different ways, unveiling different aspects
of HOI <span id="id14">[<a class="reference internal" href="#id38" title="Nicholas Timme, Wesley Alford, Benjamin Flecker, and John M Beggs. Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective. Journal of computational neuroscience, 36:119–140, 2014.">32</a>, <a class="reference internal" href="#id31" title="Thomas F Varley. Information theory for complex systems scientists. arXiv preprint arXiv:2304.12482, 2023.">35</a>]</span>. The metrics implemented in the
toolbox can be divided in two main categories: a group of metrics measures the interaction
behavior prevailing within a set of variable, <a class="reference internal" href="glossary.html#term-Network-behavior"><span class="xref std std-term">Network behavior</span></a>, another group of
metrics instead focuses on the relationship between a set of source variables and a target
one, <a class="reference internal" href="glossary.html#term-Network-encoding"><span class="xref std std-term">Network encoding</span></a>. In the following parts we are going through all the metrics
that have been developed in the toolbox, providing some insights about their theoretical
foundation and possible interpretations.</p>
<section id="network-behavior">
<h3>Network behavior<a class="headerlink" href="#network-behavior" title="Link to this heading">#</a></h3>
<section id="total-correlation">
<h4>Total correlation<a class="headerlink" href="#total-correlation" title="Link to this heading">#</a></h4>
<p>Total correlation, <a class="reference internal" href="api/generated/hoi.metrics.TC.html#hoi.metrics.TC" title="hoi.metrics.TC"><code class="xref py py-class docutils literal notranslate"><span class="pre">hoi.metrics.TC</span></code></a>, is the oldest exstension of mutual information to
an arbitrary number of variables <span id="id15">[<a class="reference internal" href="#id47" title="Milan Studen\`y and Jirina Vejnarová. The multiinformation function as a tool for measuring stochastic dependence. Learning in graphical models, pages 261–297, 1998.">28</a>, <a class="reference internal" href="#id43" title="Satosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research and development, 4(1):66–82, 1960.">36</a>]</span>.
It is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}TC(X^{n})  &amp;=  \sum_{j=1}^{n} H(X_{j}) - H(X^{n}) \\\end{split}\]</div>
<p>The total correlation quantifies the strength of collective constraints ruling the systems,
it is sentive to information shared between single variables and it can be associated with
redundancy.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to use the O-information and how it&#x27;s linked to other metrics such as the Total Correlation (TC), the Dual Total Correlation (DTC), the S-Information and the gradient O-information. We recommend reading Rosas et al. 2019 rosas2019oinfo."><img alt="" src="_images/sphx_glr_plot_oinfo_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_oinfo.html#sphx-glr-auto-examples-metrics-plot-oinfo-py"><span class="std std-ref">O-information and its derivatives for network behavior and encoding</span></a></p>
  <div class="sphx-glr-thumbnail-title">O-information and its derivatives for network behavior and encoding</div>
</div></div></section>
<section id="dual-total-correlation">
<h4>Dual Total correlation<a class="headerlink" href="#dual-total-correlation" title="Link to this heading">#</a></h4>
<p>Dual total correlation, <a class="reference internal" href="api/generated/hoi.metrics.DTC.html#hoi.metrics.DTC" title="hoi.metrics.DTC"><code class="xref py py-class docutils literal notranslate"><span class="pre">hoi.metrics.DTC</span></code></a>, is another extension of mutual information to
an arbitrary number of variables, also known as binding information and excess
entropy, <span id="id16">[<a class="reference internal" href="#id46" title="TH Sun. Linear dependence structure of the entropy space. Inf Control, 29(4):337–68, 1975.">29</a>]</span>. It quatifies the part of the joint entropy that
is shared by at least two or more variables in the following way:</p>
<div class="math notranslate nohighlight">
\[\begin{split}DTC(X^{n})  &amp;=  H(X^{n}) - \sum_{j=1}^{n} H(X_j|X_{-j}^{n}) \\
                        &amp;= \sum_{j=1}^{n} H(X_j) - (n-1)H(X^{n})\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sum_{j=1}^{n} H(X_j|X_{-j}^{n})\)</span> is the entropy of <span class="math notranslate nohighlight">\(X_j\)</span> not shared
by any other variable. This measure is higher in systems in which lower order
constraints prevails.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to use the O-information and how it&#x27;s linked to other metrics such as the Total Correlation (TC), the Dual Total Correlation (DTC), the S-Information and the gradient O-information. We recommend reading Rosas et al. 2019 rosas2019oinfo."><img alt="" src="_images/sphx_glr_plot_oinfo_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_oinfo.html#sphx-glr-auto-examples-metrics-plot-oinfo-py"><span class="std std-ref">O-information and its derivatives for network behavior and encoding</span></a></p>
  <div class="sphx-glr-thumbnail-title">O-information and its derivatives for network behavior and encoding</div>
</div></div></section>
<section id="s-information">
<h4>S information<a class="headerlink" href="#s-information" title="Link to this heading">#</a></h4>
<p>The S-information (also called exogenous information), <a class="reference internal" href="api/generated/hoi.metrics.Sinfo.html#hoi.metrics.Sinfo" title="hoi.metrics.Sinfo"><code class="xref py py-class docutils literal notranslate"><span class="pre">hoi.metrics.Sinfo</span></code></a>, is defined
as the sum between the total correlation (TC) plus the dual total
correlation (DTC), <span id="id17">[<a class="reference internal" href="#id45" title="Ryan G James, Christopher J Ellison, and James P Crutchfield. Anatomy of a bit: information in a time series observation. Chaos: An Interdisciplinary Journal of Nonlinear Science, 2011.">14</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Omega(X^{n})  &amp;=  TC(X^{n}) + DTC(X^{n}) \\
                                &amp;=  nH(X^{n}) + \sum_{j=1}^{n} [H(X_{j}) + H(
                                X_{-j}^{n})]\end{split}\]</div>
<p>It is sensitive to both redundancy and synergy, quantifying the total ammount of constraints
ruling the system under study.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to use the O-information and how it&#x27;s linked to other metrics such as the Total Correlation (TC), the Dual Total Correlation (DTC), the S-Information and the gradient O-information. We recommend reading Rosas et al. 2019 rosas2019oinfo."><img alt="" src="_images/sphx_glr_plot_oinfo_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_oinfo.html#sphx-glr-auto-examples-metrics-plot-oinfo-py"><span class="std std-ref">O-information and its derivatives for network behavior and encoding</span></a></p>
  <div class="sphx-glr-thumbnail-title">O-information and its derivatives for network behavior and encoding</div>
</div></div></section>
<section id="o-information">
<h4>O-information<a class="headerlink" href="#o-information" title="Link to this heading">#</a></h4>
<p>One prominent metric that has emerged in the pursuit of higher-order understanding is the
O-information, <a class="reference internal" href="api/generated/hoi.metrics.Oinfo.html#hoi.metrics.Oinfo" title="hoi.metrics.Oinfo"><code class="xref py py-class docutils literal notranslate"><span class="pre">hoi.metrics.Oinfo</span></code></a>. Introduced by Rosas in 2019 <span id="id18">[<a class="reference internal" href="#id29" title="Fernando E. Rosas, Pedro A. M. Mediano, Michael Gastpar, and Henrik J. Jensen. Quantifying high-order interdependencies via multivariate extensions of the mutual information. Physical Review E, 100(3):032305, September 2019. Publisher: American Physical Society. URL: https://link.aps.org/doi/10.1103/PhysRevE.100.032305 (visited on 2022-12-17), doi:10.1103/PhysRevE.100.032305.">24</a>]</span>,
O-information elegantly addresses the challenge of quantifying higher-order dependencies by
extending the concept of mutual information. Given a multiplet of <span class="math notranslate nohighlight">\(n\)</span> variables,
<span class="math notranslate nohighlight">\(X^n = \{ X_0, X_1, …, X_n \}\)</span>, its formal definition is the following:</p>
<div class="math notranslate nohighlight">
\[\Omega(X^n)= (n-2)H(X^n)+\sum_{i=1}^n \left[ H(X_i) - H(X_{-i}^n) \right]\]</div>
<p>Where <span class="math notranslate nohighlight">\(X_{-i}\)</span> is the set of all the variables in <span class="math notranslate nohighlight">\(X^n\)</span> apart from <span class="math notranslate nohighlight">\(X_i\)</span>.
The O-information can be written also as the difference between the total correlation and
the dual total correlation and reflects the balance between higher-order and lower-order
constraints among the set of variables of interest. It is shown to be a proxy of the
difference between redundancy and synergy: when the O-information of a set of variables
is positive this indicates redundancy, when it is negative, synergy. In particular when
working with big data sets it can become complicated</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Redundancy and Synergy are terms defining how variables are interacting. In this tutorial, we are going to see simple and intuitive ways of simulating redundancy and synergy in two different context :"><img alt="" src="_images/sphx_glr_plot_sim_red_syn_thumb.png" />
<p><a class="reference internal" href="auto_examples/tutorials/plot_sim_red_syn.html#sphx-glr-auto-examples-tutorials-plot-sim-red-syn-py"><span class="std std-ref">How to simulate redundancy and synergy</span></a></p>
  <div class="sphx-glr-thumbnail-title">How to simulate redundancy and synergy</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates a metric called Topological Information published by Baudot et al., 2019 baudot2019infotopo. This metric is based on differences of entropies and can be used to estimate Higher Order Interactions. One important feature of this metric relies on its ability to isolate an order. To be clear, we are going to see in this example that if we simulate a redundancy at order 3, a metric as the O-information ( hoi.metrics.Oinfo) is going to return reduandancy for all of the quadruplets that are going to include the redundant triplet. However, the hoi.metrics.InfoTopo is capable of isolating only the redundant triplet."><img alt="" src="_images/sphx_glr_plot_infotopo_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_infotopo.html#sphx-glr-auto-examples-metrics-plot-infotopo-py"><span class="std std-ref">Topological Information : conditioning on orders</span></a></p>
  <div class="sphx-glr-thumbnail-title">Topological Information : conditioning on orders</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to use the O-information and how it&#x27;s linked to other metrics such as the Total Correlation (TC), the Dual Total Correlation (DTC), the S-Information and the gradient O-information. We recommend reading Rosas et al. 2019 rosas2019oinfo."><img alt="" src="_images/sphx_glr_plot_oinfo_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_oinfo.html#sphx-glr-auto-examples-metrics-plot-oinfo-py"><span class="std std-ref">O-information and its derivatives for network behavior and encoding</span></a></p>
  <div class="sphx-glr-thumbnail-title">O-information and its derivatives for network behavior and encoding</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to estimate the confidence interval arround Higher Order Interactions. In addition, it also shows how the boostrapping can be used to fix the spatial spreading limitation of the O-information. For further information, checkout the example sphx_glr_auto_examples_metrics_plot_infotopo.py"><img alt="" src="_images/sphx_glr_plot_bootstrapping_thumb.png" />
<p><a class="reference internal" href="auto_examples/statistics/plot_bootstrapping.html#sphx-glr-auto-examples-statistics-plot-bootstrapping-py"><span class="std std-ref">Bootstrapping and confidence interval</span></a></p>
  <div class="sphx-glr-thumbnail-title">Bootstrapping and confidence interval</div>
</div></div></section>
<section id="topological-information">
<h4>Topological information<a class="headerlink" href="#topological-information" title="Link to this heading">#</a></h4>
<p>The topological information, <a class="reference internal" href="api/generated/hoi.metrics.InfoTopo.html#hoi.metrics.InfoTopo" title="hoi.metrics.InfoTopo"><code class="xref py py-class docutils literal notranslate"><span class="pre">hoi.metrics.InfoTopo</span></code></a>, a generalization of the
mutual information to higher-order, <span class="math notranslate nohighlight">\(I_k\)</span> has been introduced and presented to
test uniformity and dependence in the data <span id="id19">[<a class="reference internal" href="#id33" title="Pierre Baudot, Monica Tapia, Daniel Bennequin, and Jean-Marc Goaillard. Topological information data analysis. Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies, 2019. Number: 869. URL: https://www.mdpi.com/1099-4300/21/9/869, doi:10.3390/e21090869.">4</a>]</span>. Its formal
definition is the following:</p>
<div class="math notranslate nohighlight">
\[I_{k}(X_{1}; ...; X_{k}) = \sum_{i=1}^{k} (-1)^{i - 1} \sum_{I\subset[k];card(I)=i} H_{i}(X_{I})\]</div>
<p>Note that <span class="math notranslate nohighlight">\(I_2(X,Y) = MI(X,Y)\)</span> and that <span class="math notranslate nohighlight">\(I_3(X,Y,Z)=\Omega(X,Y,Z)\)</span>. As the
O-information this function can be interpreted in terms of redundancy and synergy, more
into details when it is positive it indicates that the system is dominated by redundancy,
when it is negative, synergy.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates a metric called Topological Information published by Baudot et al., 2019 baudot2019infotopo. This metric is based on differences of entropies and can be used to estimate Higher Order Interactions. One important feature of this metric relies on its ability to isolate an order. To be clear, we are going to see in this example that if we simulate a redundancy at order 3, a metric as the O-information ( hoi.metrics.Oinfo) is going to return reduandancy for all of the quadruplets that are going to include the redundant triplet. However, the hoi.metrics.InfoTopo is capable of isolating only the redundant triplet."><img alt="" src="_images/sphx_glr_plot_infotopo_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_infotopo.html#sphx-glr-auto-examples-metrics-plot-infotopo-py"><span class="std std-ref">Topological Information : conditioning on orders</span></a></p>
  <div class="sphx-glr-thumbnail-title">Topological Information : conditioning on orders</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to : 1. Inspect and analyse metric&#x27;s output 2. Plot Higher-Order Interactions"><img alt="" src="_images/sphx_glr_plot_inspect_results_thumb.png" />
<p><a class="reference internal" href="auto_examples/miscellaneous/plot_inspect_results.html#sphx-glr-auto-examples-miscellaneous-plot-inspect-results-py"><span class="std std-ref">Inspect and plot Higher-Order Interactions</span></a></p>
  <div class="sphx-glr-thumbnail-title">Inspect and plot Higher-Order Interactions</div>
</div></div></section>
</section>
<section id="network-encoding">
<h3>Network encoding<a class="headerlink" href="#network-encoding" title="Link to this heading">#</a></h3>
<section id="gradient-of-o-information">
<h4>Gradient of O-information<a class="headerlink" href="#gradient-of-o-information" title="Link to this heading">#</a></h4>
<p>The O-information gradient, <a class="reference internal" href="api/generated/hoi.metrics.GradientOinfo.html#hoi.metrics.GradientOinfo" title="hoi.metrics.GradientOinfo"><code class="xref py py-class docutils literal notranslate"><span class="pre">hoi.metrics.GradientOinfo</span></code></a>, has been developed to
study the contribution of one or a set of variables to the O-information of the whole
system <span id="id20">[<a class="reference internal" href="#id37" title="Tomas Scagliarini, Davide Nuzzi, Yuri Antonacci, Luca Faes, Fernando E Rosas, Daniele Marinazzo, and Sebastiano Stramaglia. Gradients of o-information: low-order descriptors of high-order dependencies. Physical Review Research, 5(1):013025, 2023.">25</a>]</span>. In this work we proposed to use this metric
to investigate the relationship between multiplets of source variables and a target
variable. Following the definition of the O-information gradient of order 1 we have:</p>
<div class="math notranslate nohighlight">
\[\partial_{target}\Omega(X^n) = \Omega(X^n, target) - \Omega(X^n)\]</div>
<p>This metric does not focus on the O-information of a group of variables, instead
it reflects the variation of O-information when the target variable is added to the group.
This allows to unveil the contribution of the target to the group of variables in terms
of O-information, providing insights about the relationship between the target and the
group of variables. Note that, when the target is statistically  independent from all
the variables of the group, the gradient of O-information is 0, when it is greater
than 0, the relation between variables and target is characterized by redundancy,
when negative, synergy.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Redundancy and Synergy are terms defining how variables are interacting. In this tutorial, we are going to see simple and intuitive ways of simulating redundancy and synergy in two different context :"><img alt="" src="_images/sphx_glr_plot_sim_red_syn_thumb.png" />
<p><a class="reference internal" href="auto_examples/tutorials/plot_sim_red_syn.html#sphx-glr-auto-examples-tutorials-plot-sim-red-syn-py"><span class="std std-ref">How to simulate redundancy and synergy</span></a></p>
  <div class="sphx-glr-thumbnail-title">How to simulate redundancy and synergy</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example compares Machine-learning and Information theoretic approaches to investigate Higher Order Interactions."><img alt="" src="_images/sphx_glr_plot_ml_vs_it_thumb.png" />
<p><a class="reference internal" href="auto_examples/tutorials/plot_ml_vs_it.html#sphx-glr-auto-examples-tutorials-plot-ml-vs-it-py"><span class="std std-ref">Machine-learning vs. Information theoretic approaches for HOI</span></a></p>
  <div class="sphx-glr-thumbnail-title">Machine-learning vs. Information theoretic approaches for HOI</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to use the O-information and how it&#x27;s linked to other metrics such as the Total Correlation (TC), the Dual Total Correlation (DTC), the S-Information and the gradient O-information. We recommend reading Rosas et al. 2019 rosas2019oinfo."><img alt="" src="_images/sphx_glr_plot_oinfo_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_oinfo.html#sphx-glr-auto-examples-metrics-plot-oinfo-py"><span class="std std-ref">O-information and its derivatives for network behavior and encoding</span></a></p>
  <div class="sphx-glr-thumbnail-title">O-information and its derivatives for network behavior and encoding</div>
</div></div></section>
<section id="redundancy-synergy-index-rsi">
<h4>Redundancy-Synergy index (RSI)<a class="headerlink" href="#redundancy-synergy-index-rsi" title="Link to this heading">#</a></h4>
<p>Another metric, proposed by Gal Chichek et al in 2001 <span id="id21">[<a class="reference internal" href="#id35" title="Gal Chechik, Amir Globerson, M Anderson, E Young, Israel Nelken, and Naftali Tishby. Group redundancy measures reveal redundancy reduction in the auditory pathway. Advances in neural information processing systems, 2001.">5</a>]</span>, is the
Redundancy-Synergy index, <a class="reference internal" href="api/generated/hoi.metrics.RSI.html#hoi.metrics.RSI" title="hoi.metrics.RSI"><code class="xref py py-class docutils literal notranslate"><span class="pre">hoi.metrics.RSI</span></code></a>, developed as an extension of mutual
information, aiming to characterize the statistical interdependencies between a group
of variables <span class="math notranslate nohighlight">\(X^n\)</span> and a target variable <span class="math notranslate nohighlight">\(Y\)</span>, in terms of redundancy and
synergy, it is computed as:</p>
<div class="math notranslate nohighlight">
\[RSI(X^n, Y) = I(X^n, Y) - \sum_{i=0}^n I(X_i,Y)\]</div>
<p>The RSI is designed to measure directly whether the sum of the information provided
separately by all the variables is greater or not with respect to the information
provided by the whole group. When RSI is positive, the whole group is more informative
than the sum of its parts separately, so the interaction between the variables and the
target is dominated by synergy. A negative RSI should instead suggest redundancy among
the variables with respect to the target.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to use and interpret the Redundancy-Synergy Index (RSI)."><img alt="" src="_images/sphx_glr_plot_rsi_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_rsi.html#sphx-glr-auto-examples-metrics-plot-rsi-py"><span class="std std-ref">Redundancy-Synergy Index</span></a></p>
  <div class="sphx-glr-thumbnail-title">Redundancy-Synergy Index</div>
</div></div></section>
<section id="synergy-and-redundancy-mmi">
<h4>Synergy and redundancy (MMI)<a class="headerlink" href="#synergy-and-redundancy-mmi" title="Link to this heading">#</a></h4>
<p>Within the broad research field of IT a growing body of literature has been produced
in the last 20 years about the fascinating concepts of synergy and redundancy. These
concepts are well defined in the framework of Partial Information Decomposition, which
aims to distinguish different “types” of information that a set of sources convey
about a target variable. In this framework, the synergy between a set of variables
refers to the presence of relationships between the target and the whole group that
cannot be seen when considering separately the single parts. Redundancy instead
refers to another phenomena, in which variables contain copies of the same
information about the target. Different definition have been provided in the
last years about these two concepts, in our work we are going to report the
simple case of the Minimum Mutual Information (MMI) <span id="id22">[<a class="reference internal" href="#id40" title="Adam B Barrett. Exploration of synergistic and redundant information sharing in static and dynamical gaussian systems. Physical Review E, 91(5):052802, 2015.">1</a>]</span>,
in which the redundancy, <a class="reference internal" href="api/generated/hoi.metrics.RedundancyMMI.html#hoi.metrics.RedundancyMMI" title="hoi.metrics.RedundancyMMI"><code class="xref py py-class docutils literal notranslate"><span class="pre">hoi.metrics.RedundancyMMI</span></code></a>, between a set
of <span class="math notranslate nohighlight">\(n\)</span> variables <span class="math notranslate nohighlight">\(X^n = \{ X_1, \ldots, X_n\}\)</span> and a target <span class="math notranslate nohighlight">\(Y\)</span> is
defined as:</p>
<div class="math notranslate nohighlight">
\[redundancy (Y, X^n) = min_{i&lt;n} I \left( Y, X_i \right)\]</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to use and interpret synergy and redundancy computed using the Minimum Mutual Information (MMI) approach to  approximate the redundancy."><img alt="" src="_images/sphx_glr_plot_syn_red_mmi_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_syn_red_mmi.html#sphx-glr-auto-examples-metrics-plot-syn-red-mmi-py"><span class="std std-ref">Redundancy and Synergy MMI</span></a></p>
  <div class="sphx-glr-thumbnail-title">Redundancy and Synergy MMI</div>
</div></div><p>When computing the redundancy in this way the definition
of synergy, <a class="reference internal" href="api/generated/hoi.metrics.SynergyMMI.html#hoi.metrics.SynergyMMI" title="hoi.metrics.SynergyMMI"><code class="xref py py-class docutils literal notranslate"><span class="pre">hoi.metrics.SynergyMMI</span></code></a>, follows:</p>
<div class="math notranslate nohighlight">
\[synergy (Y, X^n) =  I \left( Y, X^n \right) - max_{i&lt;n} I \left( Y, X^n_{ -i } \right)\]</div>
<p>Where <span class="math notranslate nohighlight">\(X^n_{-i}\)</span> is the set of variables <span class="math notranslate nohighlight">\(X^n\)</span>, excluding
the variable <span class="math notranslate nohighlight">\(i\)</span>. This metric has been proven to be accurate when
working with gaussian systems; we advise care when interpreting the
results of the redundant interactions, since the definition of
redundancy reflects simply the minimum information provided by the
source variables.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to use and interpret synergy and redundancy computed using the Minimum Mutual Information (MMI) approach to  approximate the redundancy."><img alt="" src="_images/sphx_glr_plot_syn_red_mmi_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_syn_red_mmi.html#sphx-glr-auto-examples-metrics-plot-syn-red-mmi-py"><span class="std std-ref">Redundancy and Synergy MMI</span></a></p>
  <div class="sphx-glr-thumbnail-title">Redundancy and Synergy MMI</div>
</div></div></section>
<section id="synergy-and-redundancy-integraed-information-decomposition-mmi">
<h4>Synergy and redundancy integraed Information Decomposition (MMI)<a class="headerlink" href="#synergy-and-redundancy-integraed-information-decomposition-mmi" title="Link to this heading">#</a></h4>
<p>A great deal of success has been recently obtained by different metrics focusing
on decomposing the information that two variables carry about their own
future <span id="id23">[<a class="reference internal" href="#id32" title="Pedro AM Mediano, Fernando E Rosas, Andrea I Luppi, Robin L Carhart-Harris, Daniel Bor, Anil K Seth, and Adam B Barrett. Towards an extended taxonomy of information dynamics via integrated information decomposition. arXiv preprint arXiv:2109.13186, 2021.">19</a>]</span>.
In particular, the synergy that is carried by two variables about their
joint future, has been associated with the concept of emergence and
integration of information <span id="id24">[<a class="reference internal" href="#id60" title="Andrea I Luppi, Fernando E Rosas, Pedro AM Mediano, David K Menon, and Emmanuel A Stamatakis. Information decomposition and the informational architecture of the brain. Trends in Cognitive Sciences, 2024.">18</a>, <a class="reference internal" href="#id61" title="Pedro AM Mediano, Fernando E Rosas, Andrea I Luppi, Henrik J Jensen, Anil K Seth, Adam B Barrett, Robin L Carhart-Harris, and Daniel Bor. Greater than the parts: a review of the information decomposition approach to causal emergence. Philosophical Transactions of the Royal Society A, 380(2227):20210246, 2022.">20</a>, <a class="reference internal" href="#id59" title="Fernando E Rosas, Pedro AM Mediano, Henrik J Jensen, Anil K Seth, Adam B Barrett, Robin L Carhart-Harris, and Daniel Bor. Reconciling emergences: an information-theoretic approach to identify causal emergence in multivariate data. PLoS computational biology, 16(12):e1008289, 2020.">23</a>]</span>.
Instead the redundancy as been associated with the concept of robustness,
in the sense that it refers to situation in which information
is available in different sources, making the evolution process
less vulnerable by the lost of elements  <span id="id25">[<a class="reference internal" href="#id60" title="Andrea I Luppi, Fernando E Rosas, Pedro AM Mediano, David K Menon, and Emmanuel A Stamatakis. Information decomposition and the informational architecture of the brain. Trends in Cognitive Sciences, 2024.">18</a>]</span>.
It provides already many results in simulated complex systems or in different
studies within the field of
neuroscience <span id="id26">[<a class="reference internal" href="#id62" title="Andrea I Luppi, Pedro AM Mediano, Fernando E Rosas, Judith Allanson, John D Pickard, Robin L Carhart-Harris, Guy B Williams, Michael M Craig, Paola Finoia, Adrian M Owen, and others. A synergistic workspace for human consciousness revealed by integrated information decomposition. BioRxiv, pages 2020–11, 2020.">16</a>, <a class="reference internal" href="#id59" title="Fernando E Rosas, Pedro AM Mediano, Henrik J Jensen, Anil K Seth, Adam B Barrett, Robin L Carhart-Harris, and Daniel Bor. Reconciling emergences: an information-theoretic approach to identify causal emergence in multivariate data. PLoS computational biology, 16(12):e1008289, 2020.">23</a>]</span>.
These functions allow to compute redundancy
and synergy using the approximatio of
Minimum Mutual Information (MMI) <span id="id27">[<a class="reference internal" href="#id40" title="Adam B Barrett. Exploration of synergistic and redundant information sharing in static and dynamical gaussian systems. Physical Review E, 91(5):052802, 2015.">1</a>]</span>,
in which the redundancy, <a class="reference internal" href="api/generated/hoi.metrics.RedundancyphiID.html#hoi.metrics.RedundancyphiID" title="hoi.metrics.RedundancyphiID"><code class="xref py py-class docutils literal notranslate"><span class="pre">hoi.metrics.RedundancyphiID</span></code></a>, between a couple
of variables <span class="math notranslate nohighlight">\((X, Y)\)</span> is
defined as:</p>
<div class="math notranslate nohighlight">
\[Red(X,Y) =   min \{ I(X_{t- \tau};X_t), I(X_{t-\tau};Y_t),
                    I(Y_{t-\tau}; X_t), I(Y_{t-\tau};Y_t) \}\]</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to use and interpret synergy and redundancy as defined in the Integrated Information Decomposition framework"><img alt="" src="_images/sphx_glr_plot_syn_phiID_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_syn_phiID.html#sphx-glr-auto-examples-metrics-plot-syn-phiid-py"><span class="std std-ref">Integrated Information Decomposition</span></a></p>
  <div class="sphx-glr-thumbnail-title">Integrated Information Decomposition</div>
</div></div><p>Within the MMI approximation the computation of the synergy, <a class="reference internal" href="api/generated/hoi.metrics.SynergyphiID.html#hoi.metrics.SynergyphiID" title="hoi.metrics.SynergyphiID"><code class="xref py py-class docutils literal notranslate"><span class="pre">hoi.metrics.SynergyphiID</span></code></a>,
reduces to the
following formula:</p>
<div class="math notranslate nohighlight">
\[Syn(X,Y) =  I(X_{t-\tau},Y_{t-\tau};X_{t},Y_t) -
                    max \{ I(X_{t-\tau};X_t,Y_t),
                    I(Y_{t-\tau};X_t,Y_t) \}\]</div>
<p>These two metrics are always positive and have as upper bound the value of temporal delayed
mutual information (TDMI), <span class="math notranslate nohighlight">\(I(X(t-\tau),Y(t-\tau);X(t),Y(t))\)</span>.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to use and interpret synergy and redundancy as defined in the Integrated Information Decomposition framework"><img alt="" src="_images/sphx_glr_plot_syn_phiID_thumb.png" />
<p><a class="reference internal" href="auto_examples/metrics/plot_syn_phiID.html#sphx-glr-auto-examples-metrics-plot-syn-phiid-py"><span class="std std-ref">Integrated Information Decomposition</span></a></p>
  <div class="sphx-glr-thumbnail-title">Integrated Information Decomposition</div>
</div></div></section>
</section>
</section>
</section>
<section id="bibliography">
<h1>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h1>
<div class="docutils container" id="id28">
<div role="list" class="citation-list">
<div class="citation" id="id40" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id22">1</a>,<a role="doc-backlink" href="#id27">2</a>)</span>
<p>Adam B Barrett. Exploration of synergistic and redundant information sharing in static and dynamical gaussian systems. <em>Physical Review E</em>, 91(5):052802, 2015.</p>
</div>
<div class="citation" id="id44" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">2</a><span class="fn-bracket">]</span></span>
<p>Federico Battiston, Enrico Amico, Alain Barrat, Ginestra Bianconi, Guilherme Ferraz de Arruda, Benedetta Franceschiello, Iacopo Iacopini, Sonia Kéfi, Vito Latora, Yamir Moreno, and others. The physics of higher-order interactions in complex systems. <em>Nature Physics</em>, 17(10):1093–1098, 2021.</p>
</div>
<div class="citation" id="id48" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Federico Battiston, Giulia Cencetti, Iacopo Iacopini, Vito Latora, Maxime Lucas, Alice Patania, Jean-Gabriel Young, and Giovanni Petri. Networks beyond pairwise interactions: structure and dynamics. <em>Physics Reports</em>, 874:1–92, 2020.</p>
</div>
<div class="citation" id="id33" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">4</a><span class="fn-bracket">]</span></span>
<p>Pierre Baudot, Monica Tapia, Daniel Bennequin, and Jean-Marc Goaillard. Topological information data analysis. <em>Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies</em>, 2019. Number: 869. URL: <a class="reference external" href="https://www.mdpi.com/1099-4300/21/9/869">https://www.mdpi.com/1099-4300/21/9/869</a>, <a class="reference external" href="https://doi.org/10.3390/e21090869">doi:10.3390/e21090869</a>.</p>
</div>
<div class="citation" id="id35" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">5</a><span class="fn-bracket">]</span></span>
<p>Gal Chechik, Amir Globerson, M Anderson, E Young, Israel Nelken, and Naftali Tishby. Group redundancy measures reveal redundancy reduction in the auditory pathway. <em>Advances in neural information processing systems</em>, 2001.</p>
</div>
<div class="citation" id="id65" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<p>Paweł Czyż, Frederic Grabowski, Julia Vogt, Niko Beerenwinkel, and Alexander Marx. Beyond normal: on the evaluation of mutual information estimators. <em>Advances in Neural Information Processing Systems</em>, 2024.</p>
</div>
<div class="citation" id="id52" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">7</a><span class="fn-bracket">]</span></span>
<p>Georges A Darbellay and Igor Vajda. Estimation of the information by an adaptive partitioning of the observation space. <em>IEEE Transactions on Information Theory</em>, 45(4):1315–1321, 1999.</p>
</div>
<div class="citation" id="id51" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">8</a><span class="fn-bracket">]</span></span>
<p>Dominik Endres and Peter Foldiak. Bayesian bin distribution inference and mutual information. <em>IEEE Transactions on Information Theory</em>, 51(11):3766–3779, 2005.</p>
</div>
<div class="citation" id="id53" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">9</a><span class="fn-bracket">]</span></span>
<p>Andrew M Fraser and Harry L Swinney. Independent coordinates for strange attractors from mutual information. <em>Physical review A</em>, 33(2):1134, 1986.</p>
</div>
<div class="citation" id="id39" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></span>
<p>Amir Globerson, Eran Stark, Eilon Vaadia, and Naftali Tishby. The minimum information principle and its application to neural code analysis. <em>Proceedings of the National Academy of Sciences</em>, 106(9):3490–3495, 2009.</p>
</div>
<div class="citation" id="id54" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">11</a><span class="fn-bracket">]</span></span>
<p>Nathaniel R Goodman. Statistical analysis based on a certain multivariate complex gaussian distribution (an introduction). <em>The Annals of mathematical statistics</em>, 34(1):152–177, 1963.</p>
</div>
<div class="citation" id="id41" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></span>
<p>Virgil Griffith and Christof Koch. Quantifying synergistic mutual information. In <em>Guided self-organization: inception</em>, pages 159–190. Springer, 2014.</p>
</div>
<div class="citation" id="id56" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">13</a><span class="fn-bracket">]</span></span>
<p>Robin AA Ince, Bruno L Giordano, Christoph Kayser, Guillaume A Rousselet, Joachim Gross, and Philippe G Schyns. A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula. <em>Human brain mapping</em>, 38(3):1541–1573, 2017.</p>
</div>
<div class="citation" id="id45" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">14</a><span class="fn-bracket">]</span></span>
<p>Ryan G James, Christopher J Ellison, and James P Crutchfield. Anatomy of a bit: information in a time series observation. <em>Chaos: An Interdisciplinary Journal of Nonlinear Science</em>, 2011.</p>
</div>
<div class="citation" id="id57" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">15</a><span class="fn-bracket">]</span></span>
<p>Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. <em>Physical review E</em>, 69(6):066138, 2004.</p>
</div>
<div class="citation" id="id62" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">16</a><span class="fn-bracket">]</span></span>
<p>Andrea I Luppi, Pedro AM Mediano, Fernando E Rosas, Judith Allanson, John D Pickard, Robin L Carhart-Harris, Guy B Williams, Michael M Craig, Paola Finoia, Adrian M Owen, and others. A synergistic workspace for human consciousness revealed by integrated information decomposition. <em>BioRxiv</em>, pages 2020–11, 2020.</p>
</div>
<div class="citation" id="id30" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></span>
<p>Andrea I Luppi, Pedro AM Mediano, Fernando E Rosas, Negin Holland, Tim D Fryer, John T O’Brien, James B Rowe, David K Menon, Daniel Bor, and Emmanuel A Stamatakis. A synergistic core for human brain evolution and cognition. <em>Nature Neuroscience</em>, 25(6):771–782, 2022.</p>
</div>
<div class="citation" id="id60" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id24">1</a>,<a role="doc-backlink" href="#id25">2</a>)</span>
<p>Andrea I Luppi, Fernando E Rosas, Pedro AM Mediano, David K Menon, and Emmanuel A Stamatakis. Information decomposition and the informational architecture of the brain. <em>Trends in Cognitive Sciences</em>, 2024.</p>
</div>
<div class="citation" id="id32" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id23">19</a><span class="fn-bracket">]</span></span>
<p>Pedro AM Mediano, Fernando E Rosas, Andrea I Luppi, Robin L Carhart-Harris, Daniel Bor, Anil K Seth, and Adam B Barrett. Towards an extended taxonomy of information dynamics via integrated information decomposition. <em>arXiv preprint arXiv:2109.13186</em>, 2021.</p>
</div>
<div class="citation" id="id61" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id24">20</a><span class="fn-bracket">]</span></span>
<p>Pedro AM Mediano, Fernando E Rosas, Andrea I Luppi, Henrik J Jensen, Anil K Seth, Adam B Barrett, Robin L Carhart-Harris, and Daniel Bor. Greater than the parts: a review of the information decomposition approach to causal emergence. <em>Philosophical Transactions of the Royal Society A</em>, 380(2227):20210246, 2022.</p>
</div>
<div class="citation" id="id55" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">21</a><span class="fn-bracket">]</span></span>
<p>Young-Il Moon, Balaji Rajagopalan, and Upmanu Lall. Estimation of mutual information using kernel density estimators. <em>Physical Review E</em>, 52(3):2318, 1995.</p>
</div>
<div class="citation" id="id58" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>22<span class="fn-bracket">]</span></span>
<p>Fernando E Rosas, Pedro AM Mediano, and Michael Gastpar. Characterising directed and undirected metrics of high-order interdependence. <em>arXiv preprint arXiv:2404.07140</em>, 2024.</p>
</div>
<div class="citation" id="id59" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>23<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id24">1</a>,<a role="doc-backlink" href="#id26">2</a>)</span>
<p>Fernando E Rosas, Pedro AM Mediano, Henrik J Jensen, Anil K Seth, Adam B Barrett, Robin L Carhart-Harris, and Daniel Bor. Reconciling emergences: an information-theoretic approach to identify causal emergence in multivariate data. <em>PLoS computational biology</em>, 16(12):e1008289, 2020.</p>
</div>
<div class="citation" id="id29" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">24</a><span class="fn-bracket">]</span></span>
<p>Fernando E. Rosas, Pedro A. M. Mediano, Michael Gastpar, and Henrik J. Jensen. Quantifying high-order interdependencies via multivariate extensions of the mutual information. <em>Physical Review E</em>, 100(3):032305, September 2019. Publisher: American Physical Society. URL: <a class="reference external" href="https://link.aps.org/doi/10.1103/PhysRevE.100.032305">https://link.aps.org/doi/10.1103/PhysRevE.100.032305</a> (visited on 2022-12-17), <a class="reference external" href="https://doi.org/10.1103/PhysRevE.100.032305">doi:10.1103/PhysRevE.100.032305</a>.</p>
</div>
<div class="citation" id="id37" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">25</a><span class="fn-bracket">]</span></span>
<p>Tomas Scagliarini, Davide Nuzzi, Yuri Antonacci, Luca Faes, Fernando E Rosas, Daniele Marinazzo, and Sebastiano Stramaglia. Gradients of o-information: low-order descriptors of high-order dependencies. <em>Physical Review Research</em>, 5(1):013025, 2023.</p>
</div>
<div class="citation" id="id42" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>26<span class="fn-bracket">]</span></span>
<p>Elad Schneidman, Susanne Still, Michael J Berry, William Bialek, and others. Network information and connected correlations. <em>Physical review letters</em>, 91(23):238701, 2003.</p>
</div>
<div class="citation" id="id49" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>27<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id7">2</a>,<a role="doc-backlink" href="#id12">3</a>)</span>
<p>Claude Elwood Shannon. A mathematical theory of communication. <em>The Bell system technical journal</em>, 27(3):379–423, 1948.</p>
</div>
<div class="citation" id="id47" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">28</a><span class="fn-bracket">]</span></span>
<p>Milan Studen\`y and Jirina Vejnarová. The multiinformation function as a tool for measuring stochastic dependence. <em>Learning in graphical models</em>, pages 261–297, 1998.</p>
</div>
<div class="citation" id="id46" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">29</a><span class="fn-bracket">]</span></span>
<p>TH Sun. Linear dependence structure of the entropy space. <em>Inf Control</em>, 29(4):337–68, 1975.</p>
</div>
<div class="citation" id="id34" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>30<span class="fn-bracket">]</span></span>
<p>Mónica Tapia, Pierre Baudot, Christine Formisano-Tréziny, Martial A Dufour, Simone Temporal, Manon Lasserre, Béatrice Marquèze-Pouey, Jean Gabert, Kazuto Kobayashi, and Jean-Marc Goaillard. Neurotransmitter identity and electrophysiological phenotype are genetically coupled in midbrain dopaminergic neurons. <em>Scientific reports</em>, 8(1):13637, 2018.</p>
</div>
<div class="citation" id="id50" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>31<span class="fn-bracket">]</span></span>
<p>Han Te Sun. Nonnegative entropy measures of multivariate symmetric correlations. <em>Information and Control</em>, 36:133–156, 1978.</p>
</div>
<div class="citation" id="id38" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>32<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id5">1</a>,<a role="doc-backlink" href="#id14">2</a>)</span>
<p>Nicholas Timme, Wesley Alford, Benjamin Flecker, and John M Beggs. Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective. <em>Journal of computational neuroscience</em>, 36:119–140, 2014.</p>
</div>
<div class="citation" id="id63" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>33<span class="fn-bracket">]</span></span>
<p>Nicholas Timme, Wesley Alford, Benjamin Flecker, and John M. Beggs. Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective. <em>Journal of computational neuroscience</em>, 36:119–140, 2014.</p>
</div>
<div class="citation" id="id64" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>34<span class="fn-bracket">]</span></span>
<p>Nicholas M. Timme and Christopher Lapish. A tutorial for information theory in neuroscience. <em>eNeuro</em>, 2018.</p>
</div>
<div class="citation" id="id31" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>35<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id5">1</a>,<a role="doc-backlink" href="#id14">2</a>)</span>
<p>Thomas F Varley. Information theory for complex systems scientists. <em>arXiv preprint arXiv:2304.12482</em>, 2023.</p>
</div>
<div class="citation" id="id43" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>36<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id12">2</a>,<a role="doc-backlink" href="#id15">3</a>)</span>
<p>Satosi Watanabe. Information theoretical analysis of multivariate correlation. <em>IBM Journal of research and development</em>, 4(1):66–82, 1960.</p>
</div>
<div class="citation" id="id36" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">37</a><span class="fn-bracket">]</span></span>
<p>Paul L Williams and Randall D Beer. Nonnegative decomposition of multivariate information. <em>arXiv preprint arXiv:1004.2515</em>, 2010.</p>
</div>
</div>
</div>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="glossary.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Glossary</p>
      </div>
    </a>
    <a class="right-next"
       href="api/modules.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">List of classes and functions</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Theoretical background</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-information-theoretic-measures">Core information theoretic measures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-entropy">Measuring Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-mutual-information-mi">Measuring Mutual Information (MI)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-pairwise-to-higher-order-interactions">From pairwise to higher-order interactions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#network-behavior">Network behavior</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#total-correlation">Total correlation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dual-total-correlation">Dual Total correlation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#s-information">S information</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#o-information">O-information</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#topological-information">Topological information</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#network-encoding">Network encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-o-information">Gradient of O-information</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#redundancy-synergy-index-rsi">Redundancy-Synergy index (RSI)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#synergy-and-redundancy-mmi">Synergy and redundancy (MMI)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#synergy-and-redundancy-integraed-information-decomposition-mmi">Synergy and redundancy integraed Information Decomposition (MMI)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">Bibliography</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Author name not set
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

    <hr>
    <footer>
        <div class="foot">
            <img src="https://cibul.s3.amazonaws.com/e9619f705931403093351210dc70d8ee.base.image.jpg" alt="INT" height="80">
            <img src="https://www.univ-amu.fr/system/files/2021-05/AMU%20logo.png" alt=" Aix-Marseille university" height="80">
            <img src="https://developers.google.com/open-source/gsoc/resources/downloads/GSoC-Vertical.png" alt="Gsoc" height="80">
            <img src="https://enlight-eu.org/images/logos/Logo_Gent.png" alt="Ghent" height="80">
        <br>
        <!-- <p>&copy; Copyright .</p> -->
        </div>
    </footer>

  </body>
</html>

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Overview &#8212; HOI 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css?v=995e94df" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-bootstrap.min.css?v=21c0b90a" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=f6245a2f"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'overview/ovw_theory';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="References" href="ovw_refs.html" />
    <link rel="prev" title="Overview" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
<head>
<style>
@import url('https://fonts.googleapis.com/css2?family=Maven+Pro&family=Roboto+Mono:wght@332&display=swap');

.foot {
    width: 100%;
    text-align: center;
}

/* Optionally, you can add styling to the images as well */
.foot img {
    margin: 0 10px; /* Add some space between the images if desired */
}
</style>
</head>

  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">HOI 0.0.1 documentation</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../install.html">
                        Installation
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="index.html">
                        Overview
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../api/modules.html">
                        API Reference
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../auto_examples/index.html">
                        Examples
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/brainets/hoi" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fab fa-github-square fa-lg" aria-hidden="true"></i></span>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://twitter.com/kNearNeighbors" title="Twitter" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fab fa-twitter-square fa-lg" aria-hidden="true"></i></span>
            <span class="sr-only">Twitter</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary" tabindex="0">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../install.html">
                        Installation
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="index.html">
                        Overview
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../api/modules.html">
                        API Reference
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../auto_examples/index.html">
                        Examples
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/brainets/hoi" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fab fa-github-square fa-lg" aria-hidden="true"></i></span>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://twitter.com/kNearNeighbors" title="Twitter" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fab fa-twitter-square fa-lg" aria-hidden="true"></i></span>
            <span class="sr-only">Twitter</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="ovw_refs.html">References</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Overview</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Overview</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">#</a></h1>
<p>Since the seminal work of Claude Shannon <span id="id1">[<a class="reference internal" href="ovw_refs.html#id17" title="Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423, 1948.">18</a>]</span>, in the second half of the 20th century, Information Theory (IT) has been proven to be an invaluable framework to decipher the intricate web of interactions underlying a broad range of different complex systems <span id="id2">[<a class="reference internal" href="ovw_refs.html#id14" title="Satosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research and development, 4(1):66–82, 1960.">23</a>]</span>. In this line of research, a plethora of approaches has been developed to investigate relationships between pairs of variables, shedding light on many properties of complex systems. However, a growing body of literature has recently highlighted that investigating the interactions between groups of more than 2 units, i.e. higher-order interactions (HOIs), allows to unveil effects that can be neglected by pairwise approaches <span id="id3">[<a class="reference internal" href="ovw_refs.html#id16" title="Federico Battiston, Giulia Cencetti, Iacopo Iacopini, Vito Latora, Maxime Lucas, Alice Patania, Jean-Gabriel Young, and Giovanni Petri. Networks beyond pairwise interactions: structure and dynamics. Physics Reports, 874:1–92, 2020.">2</a>]</span>. Hence, how to study HOIs has become a more and more important question in recent times <span id="id4">[]</span>. In this context, new approaches based on IT emerged to investigate HOIs in terms of information content; more into details, different metrics have been developed to estimate from the activity patterns of a set of variables, whether or not they were interacting and which kind of interaction they presented  <span id="id5">[<a class="reference internal" href="ovw_refs.html#id9" title="Nicholas Timme, Wesley Alford, Benjamin Flecker, and John M Beggs. Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective. Journal of computational neuroscience, 36:119–140, 2014.">21</a>, <a class="reference internal" href="ovw_refs.html#id3" title="Thomas F Varley. Information theory for complex systems scientists. arXiv preprint arXiv:2304.12482, 2023.">22</a>]</span>. Most of these metrics are based on the concepts of synergy and redundancy, formalized in terms of IT by the Partial Information Decomposition (PID) framework <span id="id6">[<a class="reference internal" href="ovw_refs.html#id7" title="Paul L Williams and Randall D Beer. Nonnegative decomposition of multivariate information. arXiv preprint arXiv:1004.2515, 2010.">24</a>]</span>. Even though these metrics are theoretically well defined and fascinating, when concretely using them to study and computing the higher-order structure of a system, two main problems come into play: how to estimate entropies and information from limited data set, with different hypothesis and characteristics, and how to handle the computational cost of such operations. In our work we provided a complete set of estimators to deal with different kinds of data sets, e.g. continuous or discrete, and we used the python library Jax (<a class="reference external" href="https://jax.readthedocs.io/en/latest/index.html">https://jax.readthedocs.io/en/latest/index.html</a>) to deal with the high computational cost. In the following part we will introduce the information theoretical tools necessary to understand and use the metrics developed in this toolbox. Then we will present quickly the theory behind the metrics developed, their use and possible interpretation. Finally, we are going to discuss limitations and future developments of our work.</p>
<section id="core-information-theoretic-measures">
<h2>Core information theoretic measures<a class="headerlink" href="#core-information-theoretic-measures" title="Permalink to this heading">#</a></h2>
<p>In this section, we delve into some fundamental information theoretic measures, such as Shannon Entropy and Mutual Information (MI), and their applications in the study of pairwise interactions. Besides the fact that these measures play a crucial role in various fields such as data science and machine learning, as we will see in the following parts, they serve as the building blocks for quantifying information and interactions between variables at higher-orders.</p>
<section id="measuring-entropy">
<h3>Measuring Entropy<a class="headerlink" href="#measuring-entropy" title="Permalink to this heading">#</a></h3>
<p>Shannon entropy is a fundamental concept in IT, representing the amount of uncertainty or disorder in a random variable <span id="id7">[<a class="reference internal" href="ovw_refs.html#id17" title="Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423, 1948.">18</a>]</span>. Its standard definition for a discrete random variable <span class="math notranslate nohighlight">\(X\)</span>, with probability mass function <span class="math notranslate nohighlight">\(P(X)\)</span>, is given by:</p>
<div class="math notranslate nohighlight">
\[H(X) = −\sum P(x) log_{2}(P(x))\]</div>
<p>However, estimating the probability distribution <span class="math notranslate nohighlight">\(P(X)\)</span> from data can be challenging. When dealing with a discrete variable that takes values from a limited set <span class="math notranslate nohighlight">\({x_{1}, x_{2}, ...}\)</span>, one can estimate the probability distribution by computing the frequencies of each state <span class="math notranslate nohighlight">\(x_{i}\)</span>. In this scenario we estimate the probability <span class="math notranslate nohighlight">\(P(X=x_{i}) = n_{i}/N\)</span>, where <span class="math notranslate nohighlight">\(n_{i}\)</span> is the number of occurrences <span class="math notranslate nohighlight">\(X=x_{i}\)</span> and <span class="math notranslate nohighlight">\(N\)</span> is the number of data points. This can present problems due to size effects when using a small data set and variables exploring a big set of states.</p>
<p>A more complicated and common scenario is the one of continuous variables. To estimate the entropy of a continuous variable, different methods are implemented in the toolbox:</p>
<ul class="simple">
<li><p>Binning method, that consists in binning the continuous data in a discrete set of bins. In this way, variables are discretized and the entropy can be computed as described above. This procedure can be performed in many different ways <span id="id8">[<a class="reference internal" href="ovw_refs.html#id21" title="Georges A Darbellay and Igor Vajda. Estimation of the information by an adaptive partitioning of the observation space. IEEE Transactions on Information Theory, 45(4):1315–1321, 1999.">5</a>, <a class="reference internal" href="ovw_refs.html#id20" title="Dominik Endres and Peter Foldiak. Bayesian bin distribution inference and mutual information. IEEE Transactions on Information Theory, 51(11):3766–3779, 2005.">6</a>, <a class="reference internal" href="ovw_refs.html#id22" title="Andrew M Fraser and Harry L Swinney. Independent coordinates for strange attractors from mutual information. Physical review A, 33(2):1134, 1986.">7</a>]</span>.</p></li>
<li><p>K-Nearest Neighbors (KNN), that estimates the probability distribution by considering the K nearest neighbors of each data point <span id="id9">[<a class="reference internal" href="ovw_refs.html#id26" title="Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004.">13</a>]</span>.</p></li>
<li><p>Kernel Density Estimation that uses kernel functions to estimate the probability density function, offering a smooth approximation <span id="id10">[<a class="reference internal" href="ovw_refs.html#id24" title="Young-Il Moon, Balaji Rajagopalan, and Upmanu Lall. Estimation of mutual information using kernel density estimators. Physical Review E, 52(3):2318, 1995.">14</a>]</span>.</p></li>
<li><p>The parametric estimation, that is used when the data is gaussian and allows to compute the entropy as a function of the variance <span id="id11">[<a class="reference internal" href="ovw_refs.html#id23" title="Nathaniel R Goodman. Statistical analysis based on a certain multivariate complex gaussian distribution (an introduction). The Annals of mathematical statistics, 34(1):152–177, 1963.">9</a>]</span>.</p></li>
</ul>
<p>Note that all the functions mentioned in the following part are based on the computation of  entropies, hence we advise care in the choice of the estimator to use.</p>
</section>
<section id="measuring-mutual-information-mi">
<h3>Measuring Mutual Information (MI)<a class="headerlink" href="#measuring-mutual-information-mi" title="Permalink to this heading">#</a></h3>
<p>One of the most used functions in the study of pairwise interaction is the Mutual Information (MI) that quantifies the statistical dependence or information shared between two random variables <span id="id12">[<a class="reference internal" href="ovw_refs.html#id17" title="Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423, 1948.">18</a>, <a class="reference internal" href="ovw_refs.html#id14" title="Satosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research and development, 4(1):66–82, 1960.">23</a>]</span>. It is defined mathematically using the concept of entropies. For two random variables X and Y, MI is given by:</p>
<div class="math notranslate nohighlight">
\[MI(X;Y) = H(X) + H(Y) − H(X,Y)\]</div>
<p>Where:</p>
<p><span class="math notranslate nohighlight">\(H(X)\)</span> and <span class="math notranslate nohighlight">\(H(Y)\)</span> are the entropies of individual variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.
<span class="math notranslate nohighlight">\(H(X,Y)\)</span>  is the joint entropy of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.
MI between two variables, quantifies how much knowing one variable reduces the uncertainty about the other and measures the interdependency between the two variables. If they are independent, we have <span class="math notranslate nohighlight">\(H(X,Y)=H(X)+H(Y)\)</span>, hence <span class="math notranslate nohighlight">\(MI(X,Y)=0\)</span>. Since the MI can be reduced to a signed sum of entropies, the problem of how to estimate MI from continuous data can be reconducted to the problem, discussed above, of how to estimate entropies. An estimator that has been recently developed and presents interesting properties when computing the MI is the Gaussian Copula estimator <span id="id13">[<a class="reference internal" href="ovw_refs.html#id25" title="Robin AA Ince, Bruno L Giordano, Christoph Kayser, Guillaume A Rousselet, Joachim Gross, and Philippe G Schyns. A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula. Human brain mapping, 38(3):1541–1573, 2017.">11</a>]</span>. This estimator is based on the statistical theory of copulas and is proven to provide a lower bound to the real value of MI, this is one of its main advantages: when computing MI, Gaussian copula estimator avoids false positives. Play attention to the fact that this can be mainly used to investigate relationships between two variables that are monotonic.</p>
</section>
</section>
<section id="from-pairwise-to-higher-order-interactions">
<h2>From pairwise to higher-order interactions<a class="headerlink" href="#from-pairwise-to-higher-order-interactions" title="Permalink to this heading">#</a></h2>
<p>The information theoretic metrics involved in this work are all based in principle on the concept of Shannon entropy and mutual information. Given a set of variables, a common approach to investigate their interaction is by comparing the entropy and the information of the joint probability distribution of the whole set with the entropy and information of different subsets. This can be done in many different ways, unveiling different aspects of HOIs <span id="id14">[<a class="reference internal" href="ovw_refs.html#id9" title="Nicholas Timme, Wesley Alford, Benjamin Flecker, and John M Beggs. Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective. Journal of computational neuroscience, 36:119–140, 2014.">21</a>, <a class="reference internal" href="ovw_refs.html#id3" title="Thomas F Varley. Information theory for complex systems scientists. arXiv preprint arXiv:2304.12482, 2023.">22</a>]</span>. The metrics implemented in the toolbox can be divided in two main categories: a group of metrics focus on the relationship between a set of source variables and a target one and another group measures the interactions within a set of variables. In the following part we are going through all the metrics that have been developed in the toolbox, providing some insights about their theoretical foundation and possible interpretations.</p>
<section id="o-information">
<h3>O-information<a class="headerlink" href="#o-information" title="Permalink to this heading">#</a></h3>
<p>One prominent metric that has emerged in the pursuit of higher-order understanding is the O-information. Introduced by Rosas in 2019 <span id="id15">[<a class="reference internal" href="ovw_refs.html#id2" title="Fernando E. Rosas, Pedro A. M. Mediano, Michael Gastpar, and Henrik J. Jensen. Quantifying high-order interdependencies via multivariate extensions of the mutual information. Physical Review E, 100(3):032305, September 2019. Publisher: American Physical Society. URL: https://link.aps.org/doi/10.1103/PhysRevE.100.032305 (visited on 2022-12-17), doi:10.1103/PhysRevE.100.032305.">15</a>]</span>, O-information elegantly addresses the challenge of quantifying higher-order dependencies by extending the concept of mutual information. Given a multiplet of <span class="math notranslate nohighlight">\(n\)</span> variables, <span class="math notranslate nohighlight">\(X^n = \{ X_0, X_1, …, X_n \}\)</span>, its formal definition is the following:</p>
<div class="math notranslate nohighlight">
\[\Omega(X^n)=(n-2)H(X^n)+\sum_{i=1}^n \left[ H(X_i) - H(X_{-i}^n) \right]\]</div>
<p>Where <span class="math notranslate nohighlight">\(X_{-i}\)</span> is the set of all the variables in <span class="math notranslate nohighlight">\(X^n\)</span> apart from <span class="math notranslate nohighlight">\(X_i\)</span>. The O-information can be written also as the difference between the total correlation and the dual total correlation and reflects the balance between higher-order and lower-order constraints among the set of variables of interest. It is shown to be a proxy of the difference between redundancy and synergy: when the O-information of a set of variables is positive this indicates redundancy, when it is negative, synergy. In particular when working with big data sets it can become complicated</p>
</section>
<section id="topological-information">
<h3>Topological information<a class="headerlink" href="#topological-information" title="Permalink to this heading">#</a></h3>
<p>The topological information, a generalization of the mutual information to higher-order, <span class="math notranslate nohighlight">\(I_k\)</span> has been introduced and presented to test uniformity and dependence in the data <span id="id16">[<a class="reference internal" href="ovw_refs.html#id4" title="Pierre Baudot, Monica Tapia, Daniel Bennequin, and Jean-Marc Goaillard. Topological information data analysis. Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies, 2019. Number: 869. URL: https://www.mdpi.com/1099-4300/21/9/869, doi:10.3390/e21090869.">3</a>]</span>. Its formal definition is the following:</p>
<div class="math notranslate nohighlight">
\[I_{k}(X_{1}; ...; X_{k}) = \sum_{i=1}^{k} (-1)^{i - 1} \sum_{I\subset[k];card(I)=i} H_{i}(X_{I})\]</div>
<p>Note that <span class="math notranslate nohighlight">\(I_2(X,Y) = MI(X,Y)\)</span> and that <span class="math notranslate nohighlight">\(I_3(X,Y,Z)=\Omega(X,Y,Z)\)</span>. As the O-information this function can be interpreted in terms of redundancy and synergy, more into details when it is positive it indicates that the system is dominated by redundancy, when it is negative, synergy.</p>
</section>
<section id="gradient-of-o-information">
<h3>Gradient of O-information<a class="headerlink" href="#gradient-of-o-information" title="Permalink to this heading">#</a></h3>
<p>The O-information gradient has been developed to study the contribution of one or a set of variables to the O-information of the whole system <span id="id17">[<a class="reference internal" href="ovw_refs.html#id8" title="Tomas Scagliarini, Davide Nuzzi, Yuri Antonacci, Luca Faes, Fernando E Rosas, Daniele Marinazzo, and Sebastiano Stramaglia. Gradients of o-information: low-order descriptors of high-order dependencies. Physical Review Research, 5(1):013025, 2023.">16</a>]</span>. In this work we proposed to use this metric to investigate the relationship between multiplets of source variables and a target variable. Following the definition of the O-information gradient of order 1 we have:</p>
<div class="math notranslate nohighlight">
\[\partial_{target}\Omega(X^n) = \Omega(X^n, target) - \Omega(X^n)\]</div>
<p>This metric does not focus on the O-information of a group of variables, instead it reflects the variation of O-information when the target variable is added to the group. This allows to unveil the contribution of the target to the group of variables in terms of O-information, providing insights about the relationship between the target and the group of variables. Note that, when the target is statistically  independent from all the variables of the group, the gradient of O-information is 0, when it is greater than 0, the relation between variables and target is characterized by redundancy, when negative, synergy.</p>
</section>
<section id="redundancy-synergy-index-rsi">
<h3>Redundancy-Synergy index (RSI)<a class="headerlink" href="#redundancy-synergy-index-rsi" title="Permalink to this heading">#</a></h3>
<p>Another metric, proposed by Gal Chichek et al in 2001 <span id="id18">[<a class="reference internal" href="ovw_refs.html#id6" title="Gal Chechik, Amir Globerson, M Anderson, E Young, Israel Nelken, and Naftali Tishby. Group redundancy measures reveal redundancy reduction in the auditory pathway. Advances in neural information processing systems, 2001.">4</a>]</span>, is the Redundancy-Synergy index, developed as an extension of mutual information, aiming to characterize the statistical interdependencies between a group of variables <span class="math notranslate nohighlight">\(X^n\)</span> and a target variable <span class="math notranslate nohighlight">\(Y\)</span>, in terms of redundancy and synergy, it is computed as:</p>
<div class="math notranslate nohighlight">
\[RSI(X^n, Y) = I(X^n, Y) - \sum_{i=0}^n I(X_i,Y)\]</div>
<p>The RSI is designed to measure directly whether the sum of the information provided separately by all the variables is greater or not with respect to the information provided by the whole group. When RSI is positive, the whole group is more informative than the sum of its parts separately, so the interaction between the variables and the target is dominated by synergy. A negative RSI should instead suggest redundancy among the variables with respect to the target.</p>
</section>
<section id="synergy-and-redundancy-mmi">
<h3>Synergy and redundancy (MMI)<a class="headerlink" href="#synergy-and-redundancy-mmi" title="Permalink to this heading">#</a></h3>
<p>Within the broad research field of IT a growing body of literature has been produced in the last 20 years about the fascinating concepts of synergy and redundancy. These concepts are well defined in the framework of Partial Information Decomposition, which aims to distinguish different “types” of information that a set of sources convey about a target variable. In this framework, the synergy between a set of variables refers to the presence of relationships between the target and the whole group that cannot be seen when considering separately the single parts. Redundancy instead refers to another phenomena, in which variables contain copies of the same information about the target. Different definition have been provided in the last years about these two concepts, in our work we are going to report the simple case of the Minimum Mutual Information (MMI), proposed by Barrett in <span id="id19">[<a class="reference internal" href="ovw_refs.html#id11" title="Adam B Barrett. Exploration of synergistic and redundant information sharing in static and dynamical gaussian systems. Physical Review E, 91(5):052802, 2015.">1</a>]</span>, in which the redundancy between a set of <span class="math notranslate nohighlight">\(n\)</span> variables <span class="math notranslate nohighlight">\(X^n = \{ X_1, \ldots, X_n\}\)</span> and a target <span class="math notranslate nohighlight">\(Y\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[redundancy (Y, X^n) = min_{i&lt;n} I \left( Y, X_i \right)\]</div>
<p>When computing the redundancy in this way the definition of synergy follows:</p>
<div class="math notranslate nohighlight">
\[synergy (Y, X^n) =  I \left( Y, X^n \right) - max_{i&lt;n} I \left( Y, X^n_{ -i } \right)\]</div>
<p>Where <span class="math notranslate nohighlight">\(X^n_{-i}\)</span> is the set of variables <span class="math notranslate nohighlight">\(X^n\)</span>, excluding the variable <span class="math notranslate nohighlight">\(i\)</span>. This metric has been proven to be accurate when working with gaussian systems; we advise care when interpreting the results of the redundant interactions, since the definition of redundancy reflects simply the minimum information provided by the source variables.</p>
</section>
</section>
<section id="computing-hoi-using-jax">
<h2>Computing HOI using jax<a class="headerlink" href="#computing-hoi-using-jax" title="Permalink to this heading">#</a></h2>
<p>One of the main issues in the study of the higher-order structure of complex systems is the computational cost required to investigate one by one all the multiplets of any order. When using information theoretic tools, one must consider the fact that each metric relies on a complex set of operations that have to be performed for all the multiplets of variables in the data set. The number of possible multiplets of <span class="math notranslate nohighlight">\(k\)</span> nodes in a data set grows as <span class="math notranslate nohighlight">\(\binom{n}{k}\)</span>. This means that, in a data set of <span class="math notranslate nohighlight">\(100\)</span> variables, the multiples of three nodes are <span class="math notranslate nohighlight">\(\simeq 10^5\)</span>, the multiples of 4 nodes, <span class="math notranslate nohighlight">\(\simeq 10^6\)</span> and 5 nodes, <span class="math notranslate nohighlight">\(\simeq 10^7\)</span>, etc. This leads to huge computational costs and time that can pose real problems to the study of higher-order interactions in different research fields. In this toolbox to deal with this problem, we used the recently developed library JAX, that uses XLA to compile and run your NumPy programs on GPUs and TPUs.</p>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Overview</p>
      </div>
    </a>
    <a class="right-next"
       href="ovw_refs.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">References</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-information-theoretic-measures">Core information theoretic measures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-entropy">Measuring Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-mutual-information-mi">Measuring Mutual Information (MI)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-pairwise-to-higher-order-interactions">From pairwise to higher-order interactions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#o-information">O-information</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topological-information">Topological information</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-o-information">Gradient of O-information</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#redundancy-synergy-index-rsi">Redundancy-Synergy index (RSI)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#synergy-and-redundancy-mmi">Synergy and redundancy (MMI)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-hoi-using-jax">Computing HOI using jax</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../_sources/overview/ovw_theory.rst.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

    <hr>
    <footer>
        <div class="foot">
            <img src="https://cibul.s3.amazonaws.com/e9619f705931403093351210dc70d8ee.base.image.jpg" alt="INT" height="80">
            <img src="https://www.engagement.fr/wp-content/uploads/2013/02/Aix-Marseille-Universit%C3%A9.png" alt=" Aix-Marseille university" height="80">
            <img src="https://developers.google.com/open-source/gsoc/resources/downloads/GSoC-Vertical.png" alt="Gsoc" height="80">
        <br>
        <p>&copy; Copyright BraiNets.</p>
        </div>
    </footer>

  </body>
</html>